[{"path":"/articles/classification-with-nmfkc.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Classification with NMF-LAB","text":"vignette demonstrates use nmfkc package Supervised Classification, technique referred NMF-LAB (Label-based NMF).","code":""},{"path":"/articles/classification-with-nmfkc.html","id":"how-it-works","dir":"Articles","previous_headings":"Introduction","what":"How it works","title":"Classification with NMF-LAB","text":"NMF-LAB approach treats multi-class classification matrix factorization task: Y≈XCAY \\approx X C YY (Target): One-hot encoded matrix class labels (Classes ×\\times Samples). AA (Input): Feature matrix Kernel matrix constructed features. XX (Learned Basis): mapping matrix latent factors class labels. CC (Coefficient/Parameter): Captures relationship input AA classes. Key Trade-: Linear Model (=FeaturesA = Features): Highly interpretable. CC tells exactly feature contributes class. Kernel Model (=KernelA = Kernel): Highly accurate. Handles non-linear boundaries harder interpret. explore trade-using iris dataset. First, let’s load required packages.","code":"library(nmfkc) #> Package: nmfkc (Version 0.5.8 , released on 20 12 2025 ) #> https://ksatohds.github.io/nmfkc/ library(palmerpenguins) #> Warning: package 'palmerpenguins' was built under R version 4.4.2"},{"path":"/articles/classification-with-nmfkc.html","id":"example-1-the-iris-dataset-linear-vs--kernel","dir":"Articles","previous_headings":"","what":"Example 1: The Iris Dataset (Linear vs. Kernel)","title":"Classification with NMF-LAB","text":"goal classify 3 species iris flowers (setosa, versicolor, virginica) based 4 measurements.","code":""},{"path":"/articles/classification-with-nmfkc.html","id":"data-preparation","dir":"Articles","previous_headings":"Example 1: The Iris Dataset (Linear vs. Kernel)","what":"1. Data Preparation","title":"Classification with NMF-LAB","text":"convert species labels binary class matrix YY normalize features UU.","code":"# 1. Prepare Labels (Y) label_iris <- iris$Species Y_iris <- nmfkc.class(label_iris) # One-hot encoding (3 Classes x 150 Samples) rank_iris <- length(unique(label_iris)) # Number of classes  # 2. Prepare Features (U) # Normalize features to [0, 1] range and transpose to (Features x Samples) U_iris <- t(nmfkc.normalize(iris[, -5]))"},{"path":"/articles/classification-with-nmfkc.html","id":"step-1-linear-nmf-lab-focus-on-interpretability","dir":"Articles","previous_headings":"Example 1: The Iris Dataset (Linear vs. Kernel)","what":"Step 1: Linear NMF-LAB (Focus on Interpretability)","title":"Classification with NMF-LAB","text":"First, fit Linear Model using features UU directly input matrix AA. allows us see “feature drives class?” inspecting matrix CC. Interpretation: Looking matrix CC : Class1 (Setosa): high weights Sepal.Width (2nd col). Class3 (Virginica): high weights Petal.Length (3rd col) Petal.Width (4th col). “White-box” transparency main advantage linear model. Accuracy Check: evaluate model using confusion matrix. (Note: Linear models often struggle overlapping classes like versicolor virginica.)","code":"# Use Normalized Features directly as A A_linear <- U_iris  # Fit Linear NMF-LAB res_linear <- nmfkc(Y = Y_iris, A = A_linear, rank = rank_iris, seed = 123, prefix = \"Class\") #> Y(3,150)~X(3,3)C(3,4)A(4,150)=XB(3,150)...0sec  # --- Interpretability Check --- # The matrix C (Q x R) shows the weight of each Feature (columns) for each Class (rows). # Let's look at the estimated weights: round(res_linear$C, 2) #>        Sepal.Length Sepal.Width Petal.Length Petal.Width #> Class1         0.00        0.87         0.00        0.00 #> Class2         0.03        0.00         0.57        0.00 #> Class3         0.01        0.00         0.08        0.78 pred_linear <- predict(res_linear, type = \"class\") (f_linear <- table(fitted.label = pred_linear, label = label_iris)) #>             label #> fitted.label setosa versicolor virginica #>    setosa        50          0         0 #>    virginica      0         50        50  # Calculate Accuracy (assuming diagonal correspondence) acc_linear <- sum(diag(f_linear)) / sum(f_linear) cat(paste0(\"Linear Model Accuracy: \", round(acc_linear * 100, 2), \"%\\n\")) #> Linear Model Accuracy: 66.67%"},{"path":"/articles/classification-with-nmfkc.html","id":"step-2-kernel-nmf-lab-focus-on-performance","dir":"Articles","previous_headings":"Example 1: The Iris Dataset (Linear vs. Kernel)","what":"Step 2: Kernel NMF-LAB (Focus on Performance)","title":"Classification with NMF-LAB","text":"improve accuracy, switch Kernel Model. map features high-dimensional space using Gaussian kernel. Result: accuracy jumps significantly (often >96%). kernel successfully separates complex boundaries.","code":"# 1. Optimize Kernel Width (beta) # Heuristic estimation of beta res_beta <- nmfkc.kernel.beta.nearest.med(U_iris)  # Cross-validation for fine-tuning (using generated candidates) cv_res <- nmfkc.kernel.beta.cv(Y_iris, rank = rank_iris, U = U_iris,                                 beta = res_beta$beta_candidates, plot = FALSE) #> beta=1.10348457794195...0.1sec #> beta=11.0348457794195...0sec #> beta=110.348457794195...0sec #> beta=1103.48457794195...0sec best_beta <- cv_res$beta  # 2. Fit Kernel NMF-LAB A_kernel <- nmfkc.kernel(U_iris, beta = best_beta) res_kernel <- nmfkc(Y = Y_iris, A = A_kernel, rank = rank_iris, seed = 123, prefix = \"Class\") #> Y(3,150)~X(3,3)C(3,150)A(150,150)=XB(3,150)...0sec  # 3. Prediction and Evaluation fitted_label <- predict(res_kernel, type = \"class\") (f_kernel <- table(fitted.label = fitted_label, label = label_iris)) #>             label #> fitted.label setosa versicolor virginica #>   setosa         50          0         0 #>   versicolor      0         49         4 #>   virginica       0          1        46  # Calculate Accuracy acc_kernel <- sum(diag(f_kernel)) / sum(f_kernel) cat(paste0(\"Kernel Model Accuracy: \", round(acc_kernel * 100, 2), \"%\\n\")) #> Kernel Model Accuracy: 96.67%"},{"path":"/articles/classification-with-nmfkc.html","id":"visualization-class-prototypes-basis-x","dir":"Articles","previous_headings":"Example 1: The Iris Dataset (Linear vs. Kernel)","what":"Visualization: Class Prototypes (Basis X)","title":"Classification with NMF-LAB","text":"Finally, let’s visualize Basis Matrix XX successful kernel model. Ideally, look like diagonal matrix, mapping Latent Factor specific Species.","code":"image(t(res_kernel$X)[, nrow(res_kernel$X):1],        main = \"Basis Matrix X (Kernel Model)\\nMapping Factors to Species\",       axes = FALSE, col = hcl.colors(12, \"YlOrRd\", rev = TRUE))  axis(1, at = seq(0, 1, length.out = rank_iris), labels = colnames(res_kernel$X)) axis(2, at = seq(0, 1, length.out = rank_iris), labels = rev(rownames(res_kernel$X)), las = 2) box()"},{"path":"/articles/classification-with-nmfkc.html","id":"example-2-the-palmer-penguins-dataset","dir":"Articles","previous_headings":"","what":"Example 2: The Palmer Penguins Dataset","title":"Classification with NMF-LAB","text":"Let’s apply Kernel NMF-LAB workflow classify penguin species (Adelie, Chinstrap, Gentoo), focusing Probabilistic (Soft) nature NMF classification.","code":""},{"path":"/articles/classification-with-nmfkc.html","id":"data-preparation-1","dir":"Articles","previous_headings":"Example 2: The Palmer Penguins Dataset","what":"1. Data Preparation","title":"Classification with NMF-LAB","text":"must remove rows missing values (NA) kernel matrix AA handle missing entries input features.","code":"# Load and clean data (remove rows with NAs) d_penguins <- na.omit(palmerpenguins::penguins)  # Prepare Y (Labels) label_penguins <- d_penguins$species Y_penguins <- nmfkc.class(label_penguins)  # Prepare U (Features) U_penguins <- t(nmfkc.normalize(d_penguins[, 3:6]))"},{"path":"/articles/classification-with-nmfkc.html","id":"model-fitting","dir":"Articles","previous_headings":"Example 2: The Palmer Penguins Dataset","what":"2. Model Fitting","title":"Classification with NMF-LAB","text":"use heuristic β\\beta directly quick demonstration.","code":"rank_penguins <- length(unique(label_penguins))  # 1. Heuristic beta estimation best_beta_penguins <- nmfkc.kernel.beta.nearest.med(U_penguins)$beta  # 2. Optimization A_penguins <- nmfkc.kernel(U_penguins, beta = best_beta_penguins) res_penguins <- nmfkc(Y = Y_penguins, A = A_penguins, rank = rank_penguins,                        seed = 123, prefix = \"Class\") #> Y(3,333)~X(3,3)C(3,333)A(333,333)=XB(3,333)...0.2sec"},{"path":"/articles/classification-with-nmfkc.html","id":"visualizing-soft-classification","dir":"Articles","previous_headings":"Example 2: The Palmer Penguins Dataset","what":"3. Visualizing “Soft” Classification","title":"Classification with NMF-LAB","text":"Unlike many classifiers output final label, NMF provides probability distribution classes. plot shows predicted probability penguin. Solid Blocks Color: Indicate high confidence predictions. Mixed Colors: Indicate samples model uncertain (often boundary species).","code":"# Get probabilistic predictions probs <- predict(res_penguins, type = \"prob\")  # Visualize barplot(probs, col = c(\"#FF8C00\", \"#9932CC\", \"#008B8B\"), border = NA,         main = \"Soft Classification Probabilities (Penguins)\",         xlab = \"Sample Index\", ylab = \"Probability\")  legend(\"topright\", legend = levels(label_penguins),         fill = c(\"#FF8C00\", \"#9932CC\", \"#008B8B\"), bg = \"white\", cex = 0.8)"},{"path":"/articles/classification-with-nmfkc.html","id":"evaluation","dir":"Articles","previous_headings":"Example 2: The Palmer Penguins Dataset","what":"4. Evaluation","title":"Classification with NMF-LAB","text":"Finally, calculate accuracy. } ```","code":"fitted_label_p <- predict(res_penguins, type = \"class\") (f_penguins <- table(Predicted = fitted_label_p, Actual = label_penguins)) #>            Actual #> Predicted   Adelie Chinstrap Gentoo #>   Adelie       146         0      0 #>   Chinstrap      0        68      0 #>   Gentoo         0         0    119  acc_p <- sum(diag(f_penguins)) / sum(f_penguins) cat(paste0(\"Penguins Accuracy: \", round(acc_p * 100, 2), \"%\\n\")) #> Penguins Accuracy: 100%"},{"path":"/articles/introduction-to-nmfkc.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Introduction to nmfkc","text":"Welcome nmfkc package! vignette provides beginner-friendly introduction core function, nmfkc(). Non-negative Matrix Factorization (NMF) technique decomposes large data matrix YY two smaller matrices, XX BB: Y≈XBY \\approx X B key feature NMF elements must non-negative (≥0\\ge 0). makes results intuitive, original data can understood additive combination parts. guide, cover: Basic NMF: Extracting latent topics using Movie Ratings example. Interpretation: Understanding decomposed matrices represent. Missing Values: handle predict missing data (e.g., recommendations).","code":""},{"path":"/articles/introduction-to-nmfkc.html","id":"basic-usage-analyzing-movie-ratings","dir":"Articles","previous_headings":"","what":"1. Basic Usage: Analyzing Movie Ratings","title":"Introduction to nmfkc","text":"understand NMF, let’s imagine scenario 5 Users rating 4 Movies scale 1 5. First, load package.","code":"library(nmfkc) #> Package: nmfkc (Version 0.5.8 , released on 20 12 2025 ) #> https://ksatohds.github.io/nmfkc/"},{"path":"/articles/introduction-to-nmfkc.html","id":"creating-the-data","dir":"Articles","previous_headings":"1. Basic Usage: Analyzing Movie Ratings","what":"Creating the Data","title":"Introduction to nmfkc","text":"create rating matrix Y. dataset contains two hidden genres: “Action” (Movies 1 & 2) “Romance” (Movies 3 & 4).","code":"# Rows: Users (U1-U5), Cols: Movies (M1-M4) # U1, U2, U3 prefer Action movies. # U4, U5 prefer Romance movies. Y <- matrix(   c(5, 4, 1, 1,     4, 5, 1, 2,     5, 5, 2, 2,     1, 2, 5, 4,     1, 1, 4, 5),   nrow = 5, byrow = TRUE )  # Assign names for better interpretation rownames(Y) <- paste0(\"User\", 1:5) colnames(Y) <- c(\"Action1\", \"Action2\", \"Romance1\", \"Romance2\")  # Check the data print(Y) #>       Action1 Action2 Romance1 Romance2 #> User1       5       4        1        1 #> User2       4       5        1        2 #> User3       5       5        2        2 #> User4       1       2        5        4 #> User5       1       1        4        5"},{"path":"/articles/introduction-to-nmfkc.html","id":"running-nmf","dir":"Articles","previous_headings":"1. Basic Usage: Analyzing Movie Ratings","what":"Running NMF","title":"Introduction to nmfkc","text":"use nmfkc() function decompose matrix. Since assume 2 genres (Action Romance), set rank = 2.","code":"# Run NMF with rank = 2 res <- nmfkc(Y, rank = 2, seed = 123) #> Y(5,4)~X(5,2)B(2,4)...0sec"},{"path":"/articles/introduction-to-nmfkc.html","id":"interpretation","dir":"Articles","previous_headings":"1. Basic Usage: Analyzing Movie Ratings","what":"Interpretation","title":"Introduction to nmfkc","text":"NMF decomposes YY XX (Basis) BB (Coefficient). (Note: order bases may vary depending random seed. example seed=123, Basis 1 corresponds Action Basis 2 Romance.)","code":""},{"path":"/articles/introduction-to-nmfkc.html","id":"basis-matrix-x-user-preferences","dir":"Articles","previous_headings":"1. Basic Usage: Analyzing Movie Ratings > Interpretation","what":"1. Basis Matrix X: User Preferences","title":"Introduction to nmfkc","text":"matrix XX represents “much User likes Genre (Basis).” Basis1: High values User1, User2, User3 (Action fans). Basis2: High values User4 User5 (Romance fans).","code":"# Each column represents a latent factor (Basis) res$X #>            Basis1     Basis2 #> User1 0.313397420 0.03671376 #> User2 0.304765794 0.08552167 #> User3 0.333724154 0.12109379 #> User4 0.039553910 0.37804885 #> User5 0.008558722 0.37862193"},{"path":"/articles/introduction-to-nmfkc.html","id":"coefficient-matrix-b-movie-genres","dir":"Articles","previous_headings":"1. Basic Usage: Analyzing Movie Ratings > Interpretation","what":"2. Coefficient Matrix B: Movie Genres","title":"Introduction to nmfkc","text":"matrix BB represents “Genre Movie belongs .” Basis1: High weights Action1 Action2. Basis2: High weights Romance1 Romance2. can see, NMF automatically discovered hidden structures (“Action” vs “Romance”) user preferences without explicitly told.","code":"# Each row represents a latent factor res$B #>          Action1   Action2  Romance1  Romance2 #> Basis1 14.275611 13.907355  1.264821  2.166817 #> Basis2  1.672713  3.159933 11.781222 11.778951"},{"path":"/articles/introduction-to-nmfkc.html","id":"visualization","dir":"Articles","previous_headings":"","what":"2. Visualization","title":"Introduction to nmfkc","text":"nmfkc provides tools visually diagnose model.","code":""},{"path":"/articles/introduction-to-nmfkc.html","id":"convergence-plot","dir":"Articles","previous_headings":"2. Visualization","what":"Convergence Plot","title":"Introduction to nmfkc","text":"Use plot() function check error minimized properly iterations.","code":"plot(res, main = \"Convergence Plot\")"},{"path":"/articles/introduction-to-nmfkc.html","id":"visualizing-the-reconstruction","dir":"Articles","previous_headings":"2. Visualization","what":"Visualizing the Reconstruction","title":"Introduction to nmfkc","text":"nmfkc.residual.plot() function allows compare Original Matrix (YY), Fitted Matrix (XBXB), Residuals (EE) side--side.  middle plot (Fitted Matrix) successfully captures block structure original data.","code":"# Visualize Original vs Fitted vs Residuals nmfkc.residual.plot(Y, res)"},{"path":"/articles/introduction-to-nmfkc.html","id":"handling-missing-values-imputation","dir":"Articles","previous_headings":"","what":"3. Handling Missing Values (Imputation)","title":"Introduction to nmfkc","text":"powerful feature nmfkc robustness Missing Values (NA). useful tasks like Recommendation Systems, want predict user rate movie haven’t seen yet.","code":""},{"path":"/articles/introduction-to-nmfkc.html","id":"creating-data-with-missing-values","dir":"Articles","previous_headings":"3. Handling Missing Values (Imputation)","what":"Creating Data with Missing Values","title":"Introduction to nmfkc","text":"Let’s assume User1 seen Action1 yet. set value NA.","code":"Y_missing <- Y Y_missing[\"User1\", \"Action1\"] <- NA # Introduce missing value print(Y_missing) #>       Action1 Action2 Romance1 Romance2 #> User1      NA       4        1        1 #> User2       4       5        1        2 #> User3       5       5        2        2 #> User4       1       2        5        4 #> User5       1       1        4        5"},{"path":"/articles/introduction-to-nmfkc.html","id":"running-nmf-with-nas","dir":"Articles","previous_headings":"3. Handling Missing Values (Imputation)","what":"Running NMF with NAs","title":"Introduction to nmfkc","text":"Simply pass matrix NAs nmfkc(). algorithm automatically handles ignoring missing entries optimization.","code":"res_na <- nmfkc(Y_missing, rank = 2, seed = 123) #> Notice: Missing values (NA) in Y were treated as weights=0. #> Y(5,4)~X(5,2)B(2,4)...0sec"},{"path":"/articles/introduction-to-nmfkc.html","id":"predicting-the-unknown-rating","dir":"Articles","previous_headings":"3. Handling Missing Values (Imputation)","what":"Predicting the Unknown Rating","title":"Introduction to nmfkc","text":"fitted model (X×BX \\times B) provides estimate missing entry. User1 liked Action movies, model predicted reasonably high rating (3.62) missing Action movie, closer actual rating (5) low rating.","code":"# Extract the predicted value from the fitted matrix XB predicted_rating <- res_na$XB[\"User1\", \"Action1\"] actual_rating <- Y[\"User1\", \"Action1\"] # The original hidden value (5)  cat(paste0(\"Actual Rating:    \", actual_rating, \"\\n\")) #> Actual Rating:    5 cat(paste0(\"Predicted Rating: \", round(predicted_rating, 2), \"\\n\")) #> Predicted Rating: 3.62"},{"path":"/articles/introduction-to-nmfkc.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Introduction to nmfkc","text":"nmfkc package, can easily: Decompose complex data interpretable parts (XX BB). Handle missing values robustly imputation prediction. Visualize results verify fit. advanced topics, Time Series Analysis Covariate-assisted NMF, please refer vignettes (Topic Modeling Time Series Analysis).","code":""},{"path":"/articles/nmf-sem-with-nmfkc.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"NMF-SEM with nmfkc","text":"vignette demonstrates fit NMF-SEM (Nonnegative Matrix Factorization–Structural Equation Model) implemented nmfkc package. NMF-SEM extends standard nonnegative matrix factorization introducing structural equation model latent space. Observed endogenous variables Y1Y_1 approximated Y1≈X(Θ1Y1+Θ2Y2). Y_1 \\approx X (\\Theta_1 Y_1 + \\Theta_2 Y_2). feedback operator XC1XC_1 stable, model implies equilibrium mapping Ŷ1≈(−XΘ1)−1XΘ2Y2≡MmodelY2. \\hat Y_1 \\approx (- X\\Theta_1)^{-1} X\\Theta_2 Y_2 \\equiv M_{\\mathrm{model}} Y_2.","code":""},{"path":"/articles/nmf-sem-with-nmfkc.html","id":"load-and-preprocess-data","dir":"Articles","previous_headings":"","what":"1. Load and preprocess data","title":"NMF-SEM with nmfkc","text":"","code":"library(lavaan) #> Warning: package 'lavaan' was built under R version 4.4.2 #> This is lavaan 0.6-19 #> lavaan is FREE software! Please report any bugs. data(HolzingerSwineford1939)  d <- HolzingerSwineford1939 index <- complete.cases(d) d <- d[index, ]"},{"path":"/articles/nmf-sem-with-nmfkc.html","id":"construct-exogenous-variables","dir":"Articles","previous_headings":"1. Load and preprocess data","what":"1.1 Construct exogenous variables","title":"NMF-SEM with nmfkc","text":"","code":"d$age.rev <- -(d$ageyr + d$agemo / 12) d$sex.2 <- ifelse(d$sex == 2, 1, 0) d$school.GW <- ifelse(d$school == \"Grant-White\", 1, 0)  d[, c(\"id\", \"sex\", \"ageyr\", \"agemo\", \"school\", \"grade\")] <- NULL"},{"path":"/articles/nmf-sem-with-nmfkc.html","id":"nonnegative-normalization","dir":"Articles","previous_headings":"1. Load and preprocess data","what":"1.2 Nonnegative normalization","title":"NMF-SEM with nmfkc","text":"","code":"library(nmfkc) #> Package: nmfkc (Version 0.5.8 , released on 20 12 2025 ) #> https://ksatohds.github.io/nmfkc/ d <- nmfkc::nmfkc.normalize(d)"},{"path":"/articles/nmf-sem-with-nmfkc.html","id":"split-into-endogenous-and-exogenous-blocks","dir":"Articles","previous_headings":"","what":"2. Split into endogenous and exogenous blocks","title":"NMF-SEM with nmfkc","text":"","code":"exogenous_vars <- c(\"age.rev\", \"sex.2\", \"school.GW\") endogenous_vars <- setdiff(colnames(d), exogenous_vars)  Y1 <- t(d[, endogenous_vars]) Y2 <- t(d[, exogenous_vars])"},{"path":"/articles/nmf-sem-with-nmfkc.html","id":"baseline-nmf-model","dir":"Articles","previous_headings":"","what":"3. Baseline NMF model","title":"NMF-SEM with nmfkc","text":"","code":"myepsilon <- 1e-6 Q0 <- 3  res0 <- nmfkc(   Y = Y1,   A = Y2,   Q = Q0,   epsilon = myepsilon,   X.L2.ortho = 100 ) #> Y(9,300)~X(9,3)C(3,3)A(3,300)=XB(3,300)...0.1sec  M.simple <- res0$X %*% res0$C"},{"path":"/articles/nmf-sem-with-nmfkc.html","id":"nmf-sem-estimation","dir":"Articles","previous_headings":"","what":"4. NMF-SEM estimation","title":"NMF-SEM with nmfkc","text":"","code":"p <- list(C1.L1 = 10, C2.L1 = 0.6)  res <- nmf.sem(   Y1, Y2,   rank = Q0,   X.init = res0$X,   X.L2.ortho = 100,   C1.L1 = p$C1.L1,   C2.L1 = p$C2.L1,   epsilon = myepsilon ) plot(res$objfunc.full, type = \"l\",      main = \"Objective Function\",      ylab = \"Loss\", xlab = \"Iteration\")"},{"path":"/articles/nmf-sem-with-nmfkc.html","id":"diagnostics","dir":"Articles","previous_headings":"","what":"5. Diagnostics","title":"NMF-SEM with nmfkc","text":"","code":"SC.map <- cor(as.vector(res$M.model),               as.vector(M.simple))  cat(\"Q=     \", Q0, \"\\n\") #> Q=      3 cat(\"RHO=   \", round(res$XC1.radius, 3), \"\\n\") #> RHO=    0.35 cat(\"AR=    \", round(res$amplification, 3), \"\\n\") #> AR=     1.358 cat(\"SCmap= \", round(SC.map, 3), \"\\n\") #> SCmap=  0.999 cat(\"SCcov= \", round(res$SC.cov, 3), \"\\n\") #> SCcov=  0.98 cat(\"MAE=   \", round(res$MAE, 3), \"\\n\") #> MAE=    0.181"},{"path":"/articles/nmf-sem-with-nmfkc.html","id":"visualization","dir":"Articles","previous_headings":"","what":"6. Visualization","title":"NMF-SEM with nmfkc","text":"","code":"res.dot <- nmf.sem.DOT(   res,   weight_scale = 5,   rankdir = \"TB\",   threshold = 0.01,   fill = FALSE,   cluster.box = \"none\" )  library(DiagrammeR) #> Warning: package 'DiagrammeR' was built under R version 4.4.3 grViz(res.dot)"},{"path":"/articles/timeseries-with-nmfkc.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Time Series Analysis with NMF-VAR","text":"vignette demonstrates apply nmfkc package time series data using NMF-VAR (Non-negative Matrix Factorization - Vector Autoregression) framework. approach models coefficient matrix NMF VAR process (B≈CAlagB \\approx C A_{lag}), allowing dimensionality reduction temporal modeling. Key functions nmfkc nmfkc.ar nmfkc.ar.degree.cv directly support R’s native ts objects, preserving time series properties automatically. cover two scenarios: Univariate AR: Forecasting airline passengers (AirPassengers). Multivariate VAR: Analyzing macroeconomic indicators (Canada). First, let’s load necessary packages.","code":"library(nmfkc) #> Package: nmfkc (Version 0.5.8 , released on 20 12 2025 ) #> https://ksatohds.github.io/nmfkc/ library(vars) # For Canada dataset #> Warning: package 'vars' was built under R version 4.4.3 #> Loading required package: MASS #> Loading required package: strucchange #> Warning: package 'strucchange' was built under R version 4.4.2 #> Loading required package: zoo #> Warning: package 'zoo' was built under R version 4.4.3 #>  #> Attaching package: 'zoo' #> The following objects are masked from 'package:base': #>  #>     as.Date, as.Date.numeric #> Loading required package: sandwich #> Warning: package 'sandwich' was built under R version 4.4.2 #> Loading required package: urca #> Warning: package 'urca' was built under R version 4.4.3 #> Loading required package: lmtest #> Warning: package 'lmtest' was built under R version 4.4.2"},{"path":"/articles/timeseries-with-nmfkc.html","id":"example-1-univariate-autoregression-with-airpassengers","dir":"Articles","previous_headings":"","what":"Example 1: Univariate Autoregression with AirPassengers","title":"Time Series Analysis with NMF-VAR","text":"AirPassengers dataset contains monthly international airline passenger numbers. model univariate time series using autoregressive (AR) model.","code":""},{"path":"/articles/timeseries-with-nmfkc.html","id":"data-preparation","dir":"Articles","previous_headings":"Example 1: Univariate Autoregression with AirPassengers","what":"1. Data Preparation","title":"Time Series Analysis with NMF-VAR","text":"simply log-transform ts object stabilize variance. need manually convert matrix create time vectors; nmfkc functions handle ts objects directly.","code":"# Load and transform the ts object d_air <- AirPassengers d_air_log <- log10(d_air) # Still a ts object"},{"path":"/articles/timeseries-with-nmfkc.html","id":"model-selection-lag-order","dir":"Articles","previous_headings":"Example 1: Univariate Autoregression with AirPassengers","what":"2. Model Selection (Lag Order)","title":"Time Series Analysis with NMF-VAR","text":"use Cross-Validation find optimal degree (lag order). passing ts object d_air_log, function automatically handles time dimension.","code":"# Evaluate lag orders from 1 to 14 # Note: ts objects are automatically transposed to (Variables x Time) internally cv_res <- nmfkc.ar.degree.cv(d_air_log, rank = 1, degree = 1:14, epsilon=1e-6, maxit=500000) #> degree=1...0.2sec #> degree=2...0.8sec #> degree=3...0.8sec #> degree=4...0.8sec #> degree=5...0.8sec #> degree=6...0.8sec #> degree=7...0.8sec #> degree=8...0.8sec #> degree=9...0.9sec #> degree=10...0.9sec #> degree=11...1sec #> degree=12...0.9sec #> degree=13...1sec #> degree=14...1sec # Check the optimal degree cv_res$degree #> [1] 12  # For this example, we will proceed with D=12 (capturing monthly seasonality) D <- 12"},{"path":"/articles/timeseries-with-nmfkc.html","id":"model-fitting","dir":"Articles","previous_headings":"Example 1: Univariate Autoregression with AirPassengers","what":"3. Model Fitting","title":"Time Series Analysis with NMF-VAR","text":"construct observation matrix Y covariate matrix (lagged Y) using nmfkc.ar(). returned matrices Y time-based column names derived ts object.","code":"# Create matrices for the AR(12) model a_air <- nmfkc.ar(d_air_log, degree = D, intercept = TRUE)  # Fit the NMF-AR model (Rank=1 for univariate) res_air <- nmfkc(Y = a_air$Y, A = a_air$A, rank = 1, epsilon = 1e-6, maxit=500000) #> Y(1,132)~X(1,1)C(1,13)A(13,132)=XB(1,132)...0.2sec  # Check goodness of fit res_air$r.squared #> [1] 0.9798257  # Check for stationarity (spectral radius < 1) nmfkc.ar.stationarity(res_air) #> $spectral.radius #> [1] 0.9983758 #>  #> $stationary #> [1] TRUE"},{"path":"/articles/timeseries-with-nmfkc.html","id":"forecasting","dir":"Articles","previous_headings":"Example 1: Univariate Autoregression with AirPassengers","what":"4. Forecasting","title":"Time Series Analysis with NMF-VAR","text":"can forecast future values using fitted model. nmfkc.ar.predict uses time properties stored model generate correct future time sequence.","code":"# Forecast next 2 years (24 months) h <- 24 pred_res <- nmfkc.ar.predict(x = res_air, Y = a_air$Y, n.ahead = h)  # Convert predictions back to original scale pred_val <- 10^as.vector(pred_res$pred) pred_time <- pred_res$time # Future time points generated by the function  # --- Plotting --- # Setup plot range xlim_range <- range(c(time(d_air), pred_time)) ylim_range <- range(c(d_air, pred_val))  # 1. Observed data (Black) plot(d_air, type = \"l\", col = \"black\",       xlim = xlim_range, ylim = ylim_range, lwd = 1,      xlab = \"Year\", ylab = \"Air Passengers\", main = \"NMF-VAR Forecast (h=24)\")  # 2. Fitted values during training (Red) # a_air$Y has column names as time strings; we parse them for plotting fitted_time <- as.numeric(colnames(res_air$XB)) lines(fitted_time, 10^as.vector(res_air$XB), col = \"red\", lwd = 2)  # 3. Forecast (Blue) # Connect the last observed point to the first forecast for a continuous line last_t <- tail(as.numeric(time(d_air)), 1) last_y <- tail(as.vector(d_air), 1) lines(c(last_t, pred_time), c(last_y, pred_val), col = \"blue\", lwd = 2, lty = 2)  # Add legend legend(\"topleft\", legend = c(\"Observed\", \"Fitted\", \"Forecast\"),        col = c(\"black\", \"red\", \"blue\"), lty = c(1, 1, 2), lwd = 2)"},{"path":"/articles/timeseries-with-nmfkc.html","id":"example-2-vector-autoregression-with-canada-dataset","dir":"Articles","previous_headings":"","what":"Example 2: Vector Autoregression with Canada Dataset","title":"Time Series Analysis with NMF-VAR","text":"Canada dataset contains four quarterly macroeconomic variables. ’ll use multivariate NMF-VAR model analyze relationship variables.","code":""},{"path":"/articles/timeseries-with-nmfkc.html","id":"data-preparation-1","dir":"Articles","previous_headings":"Example 2: Vector Autoregression with Canada Dataset","what":"1. Data Preparation","title":"Time Series Analysis with NMF-VAR","text":"take first difference achieve stationarity, normalize data [0,1][0, 1], transpose Variables x Time format required NMF.","code":"# Load, difference, and normalize d0_canada <- Canada dd_canada <- apply(d0_canada, 2, diff) # Returns a matrix (Time x Vars) dn_canada <- nmfkc.normalize(dd_canada)  # Transpose to (Variables x Time) for NMF Y0_canada <- t(dn_canada)  # Create matrices for VAR(1) a_canada <- nmfkc.ar(Y0_canada, degree = 1, intercept = TRUE)"},{"path":"/articles/timeseries-with-nmfkc.html","id":"model-fitting-1","dir":"Articles","previous_headings":"Example 2: Vector Autoregression with Canada Dataset","what":"2. Model Fitting","title":"Time Series Analysis with NMF-VAR","text":"fit model rank = 2 identify two latent economic conditions driving four variables.","code":"# Fit the NMF-VAR model res_canada <- nmfkc(Y = a_canada$Y, A = a_canada$A, rank = 2, prefix = \"Condition\", epsilon = 1e-6) #> Y(4,82)~X(4,2)C(2,5)A(5,82)=XB(2,82)...0sec  # R-squared and Stationarity res_canada$r.squared #> [1] 0.5994663 nmfkc.ar.stationarity(res_canada) #> $spectral.radius #> [1] 0.7781102 #>  #> $stationary #> [1] TRUE"},{"path":"/articles/timeseries-with-nmfkc.html","id":"latent-structure-causal-graph","dir":"Articles","previous_headings":"Example 2: Vector Autoregression with Canada Dataset","what":"3. Latent Structure & Causal Graph","title":"Time Series Analysis with NMF-VAR","text":"can visualize two latent conditions change time.  Finally, can generate DOT script visualize Granger causality (relationships) variables inferred model.","code":"# Visualize soft clustering of time trends barplot(res_canada$B.prob, col = c(2, 3), border = NA,         main = \"Soft Clustering of Economic Conditions\",         xlab = \"Time\", ylab = \"Probability\",         names.arg = colnames(a_canada$Y), las=2, cex.names = 0.5) legend(\"topright\", legend = colnames(res_canada$X), fill = c(2, 3), bg = \"white\") # Generate DOT script for graph visualization dot_script <- nmfkc.ar.DOT(res_canada, intercept = TRUE, threshold=0.01)  # Visualize the graph using DiagrammeR if (requireNamespace(\"DiagrammeR\", quietly = TRUE)) {   DiagrammeR::grViz(dot_script) } else {   message(\"Please install 'DiagrammeR' to visualize the graph.\")   cat(substr(dot_script, 1, 300), \"...\\n(truncated)\") }"},{"path":"/articles/topic-modeling-with-nmfkc.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Topic Modeling with nmfkc","text":"Non-negative Matrix Factorization (NMF) powerful technique topic modeling. decomposing document-term matrix, can simultaneously discover latent Topics (clusters words) Trends (proportions documents). vignette demonstrates use nmfkc package analyze U.S. presidential inaugural addresses using quanteda package. cover: Data Preparation: Converting text data matrix format suitable nmfkc. Rank Selection: Determining optimal number topics using robust Cross-Validation. Standard NMF: Extracting basic topics interpreting key words. Kernel NMF: Modeling temporal evolution topics using “Year” covariate. First, let’s load necessary packages.","code":"library(nmfkc) #> Package: nmfkc (Version 0.5.8 , released on 20 12 2025 ) #> https://ksatohds.github.io/nmfkc/ library(quanteda) #> Warning: package 'quanteda' was built under R version 4.4.2 #> Package version: 4.2.0 #> Unicode version: 15.1 #> ICU version: 74.1 #> Parallel computing: 32 of 32 threads used. #> See https://quanteda.io for tutorials and examples."},{"path":"/articles/topic-modeling-with-nmfkc.html","id":"data-preparation","dir":"Articles","previous_headings":"","what":"1. Data Preparation","title":"Topic Modeling with nmfkc","text":"create Document-Feature Matrix (DFM) rows represent documents columns represent words.","code":"# Load the corpus from quanteda corp <- corpus(data_corpus_inaugural)  # Preprocessing: tokenize, remove stopwords, and punctuation tok <- tokens(corp, remove_punct = TRUE) tok <- tokens_remove(tok, pattern = stopwords(\"en\", source = \"snowball\"))  # Create DFM and filter df <- dfm(tok) df <- dfm_select(df, min_nchar = 3) # Remove short words (<= 2 chars) df <- dfm_trim(df, min_termfreq = 50) # Remove rare words (appearing < 50 times)  # --- CRITICAL STEP --- # quanteda's DFM is (Documents x Words). # nmfkc expects (Words x Documents). # We must transpose the matrix. d <- as.matrix(df) Y <- t(d)  dim(Y) # Features (Words) x Samples (Documents) #> [1] 204  59"},{"path":"/articles/topic-modeling-with-nmfkc.html","id":"rank-selection-number-of-topics","dir":"Articles","previous_headings":"","what":"2. Rank Selection (Number of Topics)","title":"Topic Modeling with nmfkc","text":"fitting model, need decide number topics (rankrank). nmfkc.rank() function helps us choose appropriate rank. , set save.time = FALSE perform Element-wise Cross-Validation (Wold’s CV). method randomly holds individual matrix elements evaluates well model predicts , providing robust measure rank selection (though takes computation time).  Looking diagnostics (e.g., minimum ECV Sigma, elbow R-squared curve, high Cophenetic Correlation), let’s assume Rank = 3 reasonable choice overview.","code":"# Evaluate ranks from 2 to 6 # save.time=FALSE enables the robust Element-wise CV nmfkc.rank(Y, rank = 2:6, save.time = FALSE) #> Y(204,59)~X(204,2)B(2,59)...0sec #> Y(204,59)~X(204,3)B(3,59)...0sec #> Y(204,59)~X(204,4)B(4,59)...0sec #> Y(204,59)~X(204,5)B(5,59)...0sec #> Y(204,59)~X(204,6)B(6,59)...0sec #> Running Element-wise CV (this may take time)... #> Performing Element-wise CV for Q = 2,3,4,5,6 (5-fold)... #> Y(204,59)~X(204,2)B(2,59)...0sec #> Y(204,59)~X(204,2)B(2,59)...0sec #> Y(204,59)~X(204,2)B(2,59)...0sec #> Y(204,59)~X(204,2)B(2,59)...0sec #> Y(204,59)~X(204,2)B(2,59)...0sec #> Y(204,59)~X(204,3)B(3,59)...0sec #> Y(204,59)~X(204,3)B(3,59)...0sec #> Y(204,59)~X(204,3)B(3,59)...0sec #> Y(204,59)~X(204,3)B(3,59)...0sec #> Y(204,59)~X(204,3)B(3,59)...0sec #> Y(204,59)~X(204,4)B(4,59)...0sec #> Y(204,59)~X(204,4)B(4,59)...0sec #> Y(204,59)~X(204,4)B(4,59)...0sec #> Y(204,59)~X(204,4)B(4,59)...0sec #> Y(204,59)~X(204,4)B(4,59)...0sec #> Y(204,59)~X(204,5)B(5,59)...0sec #> Y(204,59)~X(204,5)B(5,59)...0sec #> Y(204,59)~X(204,5)B(5,59)...0sec #> Y(204,59)~X(204,5)B(5,59)...0sec #> Y(204,59)~X(204,5)B(5,59)...0sec #> Y(204,59)~X(204,6)B(6,59)...0sec #> Y(204,59)~X(204,6)B(6,59)...0.1sec #> Y(204,59)~X(204,6)B(6,59)...0sec #> Y(204,59)~X(204,6)B(6,59)...0sec #> Y(204,59)~X(204,6)B(6,59)...0sec #> $rank.best #> [1] 2 #>  #> $criteria #>   rank r.squared       ICp      AIC      BIC B.prob.sd.min B.prob.entropy.mean #> 1    2 0.5533924  37.65518 18393.35 22268.68     0.3457205           0.5845892 #> 2    3 0.5941030  55.66664 17767.02 23580.01     0.1536632           0.6637765 #> 3    4 0.6285527  73.68498 17223.59 24974.23     0.1323326           0.6661412 #> 4    5 0.6565263  91.71388 16807.10 26495.41     0.1207275           0.6558215 #> 5    6 0.6830576 109.74059 16364.42 27990.39     0.1291896           0.6490900 #>   B.prob.max.mean       ARI silhouette      CPCC  dist.cor sigma.ecv #> 1       0.8190118        NA  0.7506465 0.9306425 0.8886148  2.284836 #> 2       0.6797261 0.7224439  0.5497821 0.9213624 0.9084099  2.421535 #> 3       0.6079841 0.5299410  0.4586054 0.9082198 0.9296355  2.738010 #> 4       0.5699106 0.6070068  0.3781011 0.9246659 0.9291057  2.950237 #> 5       0.5258122 0.6750738  0.3171195 0.8751251 0.9338937  3.024230"},{"path":"/articles/topic-modeling-with-nmfkc.html","id":"standard-nmf","dir":"Articles","previous_headings":"","what":"3. Standard NMF","title":"Topic Modeling with nmfkc","text":"fit standard NMF model (Y≈XBY \\approx XB) rank = 3. context topic modeling: XX (Basis Matrix): Represents Topics (distribution words). BB (Coefficient Matrix): Represents Trends (distribution topics across documents).","code":"rank <- 3 # Set seed for reproducibility res_std <- nmfkc(Y, rank = rank, seed = 123, prefix = \"Topic\") #> Y(204,59)~X(204,3)B(3,59)...0sec  # Check Goodness of Fit (R-squared) res_std$r.squared #> [1] 0.594103"},{"path":"/articles/topic-modeling-with-nmfkc.html","id":"interpreting-topics-keywords","dir":"Articles","previous_headings":"3. Standard NMF","what":"Interpreting Topics (Keywords)","title":"Topic Modeling with nmfkc","text":"can identify meaning topic looking words highest weights basis matrix X. (Note: Interpretation depends result. example, one topic might contain words like “government, people, states” (Political), another might “world, peace, freedom” (International).)","code":"# Extract top 10 words for each topic from X.prob (normalized X) Xp <- res_std$X.prob for(k in 1:rank){   top_words <- names(sort(Xp[,k], decreasing = TRUE)[1:10])   cat(paste0(\"Topic \", k, \" Keywords: \", paste(top_words, collapse = \", \"), \"\\n\")) } #> Topic 1 Keywords: however, character, executive, citizen, power, principle, constitution, given, might, become #> Topic 2 Keywords: policy, commerce, protection, revenue, question, business, laws, trade, respect, duties #> Topic 3 Keywords: today, americans, america, help, together, know, live, let, lives, earth"},{"path":"/articles/topic-modeling-with-nmfkc.html","id":"kernel-nmf-temporal-topic-evolution","dir":"Articles","previous_headings":"","what":"4. Kernel NMF: Temporal Topic Evolution","title":"Topic Modeling with nmfkc","text":"One unique features nmfkc Kernel NMF. standard NMF, order documents ignored; speech treated independently. However, inaugural addresses strong temporal component. using “Year” covariate, can smooth topic proportions time see historical shifts.","code":""},{"path":"/articles/topic-modeling-with-nmfkc.html","id":"optimizing-the-kernel-parameter","dir":"Articles","previous_headings":"4. Kernel NMF: Temporal Topic Evolution","what":"Optimizing the Kernel Parameter","title":"Topic Modeling with nmfkc","text":"construct covariate matrix U using year address. find optimal kernel bandwidth (beta) using Cross-Validation.","code":"# Covariate: Year of the address years <- as.numeric(substring(names(data_corpus_inaugural), 1, 4)) U <- t(as.matrix(years))  # Optimize beta (Gaussian Kernel width) # We test a specific range of betas to locate the minimum CV error. beta_candidates <- c(0.2, 0.5, 1, 2, 5) / 10000  # Run CV to find the best beta # Note: We use the same rank (Q=3) as selected above. cv_res <- nmfkc.kernel.beta.cv(Y, rank = rank, U = U, beta = beta_candidates, plot = FALSE) #> beta=2e-05...0.2sec #> beta=5e-05...0.1sec #> beta=1e-04...0.1sec #> beta=2e-04...0sec #> beta=5e-04...0sec best_beta <- cv_res$beta print(best_beta) #> [1] 2e-04"},{"path":"/articles/topic-modeling-with-nmfkc.html","id":"fitting-kernel-nmf","dir":"Articles","previous_headings":"4. Kernel NMF: Temporal Topic Evolution","what":"Fitting Kernel NMF","title":"Topic Modeling with nmfkc","text":"Now fit model using kernel matrix . enforces documents close time (similar years) similar topic distributions.","code":"# Create Kernel Matrix A <- nmfkc.kernel(U, beta = best_beta)  # Fit NMF with Kernel Covariates res_ker <- nmfkc(Y, A = A, rank = rank, seed = 123, prefix = \"Topic\") #> Y(204,59)~X(204,3)C(3,59)A(59,59)=XB(3,59)...0sec"},{"path":"/articles/topic-modeling-with-nmfkc.html","id":"visualization-standard-vs-kernel-nmf","dir":"Articles","previous_headings":"4. Kernel NMF: Temporal Topic Evolution","what":"Visualization: Standard vs Kernel NMF","title":"Topic Modeling with nmfkc","text":"Let’s compare topic proportions change time. Standard NMF (Top): Shows noisy fluctuations. captures specific content speech misses larger historical context. Kernel NMF (Bottom): Reveals smooth historical trends, showing themes like “Nation Building” vs “Global Affairs” evolved centuries.","code":"par(mfrow = c(2, 1), mar = c(4, 4, 2, 1))  # Prepare Axis Labels (Rounded to integers) at_points <- seq(1, ncol(Y), length.out = 10) labels_years <- round(seq(min(years), max(years), length.out = 10))  # 1. Standard NMF (Noisy) barplot(res_std$B.prob, col = 2:(rank+1), border = NA, xaxt='n',         main = \"Standard NMF: Topic Proportions (Noisy)\", ylab = \"Probability\") axis(1, at = at_points, labels = labels_years)  # 2. Kernel NMF (Smooth trend) barplot(res_ker$B.prob, col = 2:(rank+1), border = NA, xaxt='n',         main = \"Kernel NMF: Temporal Topic Evolution (Smooth)\", ylab = \"Probability\") axis(1, at = at_points, labels = labels_years)  # Legend legend(\"topright\", legend = paste(\"Topic\", 1:rank), fill = 2:(rank+1), bg=\"white\", cex=0.8)"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Kenichi Satoh. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Satoh K (2025). “Applying non-negative matrix factorization covariates label matrix classification.” doi:https://arxiv.org/abs/2510.10375. Satoh K (2025). “Applying non-negative Matrix Factorization Covariates Multivariate Time Series Data Vector Autoregression Model.” Japanese Journal Statistics Data Science. doi:10.1007/s42081-025-00314-0. Satoh K (2024). “Applying Non-negative Matrix Factorization Covariates Longitudinal Data Growth Curve Model.” arXiv. doi:https://arxiv.org/abs/2403.05359. Satoh K (2023). “Non-negative Matrix Factorization Using Gaussian Kernels Covariates.” Japanese Journal Applied Statistics, 52(2), -15. doi:10.5023/jappstat.52.59.","code":"@Misc{,   title = {Applying non-negative matrix factorization with covariates to label matrix for classification},   author = {Kenichi Satoh},   journal = {arXiv},   year = {2025},   doi = {https://arxiv.org/abs/2510.10375}, } @Article{,   title = {Applying non-negative Matrix Factorization with Covariates to Multivariate Time Series Data as a Vector Autoregression Model},   author = {Kenichi Satoh},   journal = {Japanese Journal of Statistics and Data Science},   year = {2025},   doi = {https://doi.org/10.1007/s42081-025-00314-0}, } @Article{,   title = {Applying Non-negative Matrix Factorization with Covariates to the Longitudinal Data as Growth Curve Model},   author = {Kenichi Satoh},   journal = {arXiv},   year = {2024},   doi = {https://arxiv.org/abs/2403.05359}, } @Article{,   title = {On Non-negative Matrix Factorization Using Gaussian Kernels as Covariates},   author = {Kenichi Satoh},   journal = {Japanese Journal of Applied Statistics},   year = {2023},   volume = {52},   number = {2},   pages = {-15},   doi = {https://doi.org/10.5023/jappstat.52.59}, }"},{"path":"/index.html","id":"nmfkc-non-negative-matrix-factorization-with-kernel-covariates","dir":"","previous_headings":"","what":"Non-Negative Matrix Factorization with Kernel Covariates","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"nmfkc R package extends Non-negative Matrix Factorization (NMF) incorporating covariates using kernel methods. supports advanced features like rank selection via cross-validation, time-series modeling (NMF-VAR), supervised classification (NMF-LAB), structural equation modeling equilibrium interpretation (NMF-SEM).","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"can install nmfkc package directly GitHub.","code":""},{"path":"/index.html","id":"quick-installation-no-vignettes","dir":"","previous_headings":"","what":"Quick Installation (No Vignettes)","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"fastest installation without building package vignettes (tutorials), use following commands.","code":"# If you don't have the 'remotes' package installed, uncomment the line below: # install.packages(\"remotes\")  # Install the package without building vignettes (default behavior) remotes::install_github(\"ksatohds/nmfkc\")  # Load the package library(nmfkc)"},{"path":"/index.html","id":"detailed-installation-with-vignettes","dir":"","previous_headings":"","what":"Detailed Installation (With Vignettes)","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"Recommended want view tutorials locally.","code":"# Install the package and build the vignettes remotes::install_github(\"ksatohds/nmfkc\", build_vignettes = TRUE)  # Load the package library(nmfkc)  # You can view the vignettes with: browseVignettes(\"nmfkc\")"},{"path":"/index.html","id":"alternative-download-source-archive","dir":"","previous_headings":"","what":"Alternative Download (Source Archive)","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"Package Archive File (.tar.gz) also available Dropbox.","code":""},{"path":"/index.html","id":"package-explanation-video","dir":"","previous_headings":"","what":"Package Explanation Video","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"short video explaining package can found : YouTube","code":""},{"path":"/index.html","id":"help-and-usage","dir":"","previous_headings":"","what":"Help and Usage","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"","code":"browseVignettes(\"nmfkc\") ls(\"package:nmfkc\") ?nmfkc"},{"path":"/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"Please use following command cite nmfkc package:","code":"library(nmfkc) citation(\"nmfkc\")"},{"path":[]},{"path":"/index.html","id":"functions","dir":"","previous_headings":"","what":"Functions","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"nmfkc package provides comprehensive suite functions performing, diagnosing, visualizing Non-negative Matrix Factorization Kernel Covariates.","code":""},{"path":"/index.html","id":"id_1-core-algorithm","dir":"","previous_headings":"","what":"1. Core Algorithm","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"heart package nmfkc function, estimates basis matrix XX parameter matrix CC (B=CAB=CA) based observation YY covariates AA. nmfkc: Fits NMF model (Y≈XCAY \\approx XCA) using multiplicative update rules. Supports missing values (NA) observation weights.","code":""},{"path":"/index.html","id":"nmf-sem-structural-equation-modeling","dir":"","previous_headings":"1. Core Algorithm","what":"NMF-SEM (Structural Equation Modeling)","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"NMF-SEM fits nonnegative structural equation model endogenous feedback equilibrium (Leontief-type) mapping: Y1≈X(Θ1Y1+Θ2Y2),Mmodel=(−XΘ1)−1XΘ2. Y_1 \\approx X(\\Theta_1 Y_1 + \\Theta_2 Y_2), \\qquad M_{model} = (- X\\Theta_1)^{-1} X\\Theta_2. nmf.sem: Fits NMF-SEM model returns latent basis XX, feedback Θ1\\Theta_1 (C1), exogenous loading Θ2\\Theta_2 (C2), equilibrium mapping M.model. nmf.sem.cv: Predictive K-fold CV selecting NMF-SEM hyperparameters based MAE Ŷ1=MmodelY2\\hat Y_1 = M_{model} Y_2. nmf.sem.DOT: Generates Graphviz/DOT code visualize estimated structure (feedback exogenous paths).","code":""},{"path":"/index.html","id":"id_2-model-selection--diagnostics","dir":"","previous_headings":"","what":"2. Model Selection & Diagnostics","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"Tools determine optimal number bases (Rank QQ) evaluate model performance. Elbow Method: Based curvature R2R^2. Wold’s CV: Based Element-wise Cross-Validation (Min RMSE). Also computes stability metrics like Cophenetic Correlation Coefficient (CPCC) Silhouette scores. nmfkc.ecv: Performs Element-wise Cross-Validation (Wold’s CV). Randomly masks elements matrix evaluate structural reconstruction error. Theoretically robust rank selection. nmfkc.cv: Performs Column-wise Cross-Validation. Useful evaluating prediction performance new (unseen) individuals. nmfkc.residual.plot: Visualizes original matrix, fitted matrix, residual matrix side--side check randomness errors.","code":""},{"path":"/index.html","id":"id_3-covariate-engineering","dir":"","previous_headings":"","what":"3. Covariate Engineering","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"Functions construct covariate matrix AA raw features UU. nmfkc.kernel: Generates kernel matrix (e.g., Gaussian, Polynomial) covariates. nmfkc.ar: Constructs lagged matrices Vector Autoregression (NMF-VAR) models. nmfkc.class: Converts categorical vectors one-hot encoded matrices classification tasks (NMF-LAB).","code":""},{"path":"/index.html","id":"id_4-parameter-tuning","dir":"","previous_headings":"","what":"4. Parameter Tuning","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"Helper functions optimize hyperparameters rank. nmfkc.kernel.beta.cv: Optimizes Gaussian kernel width (β\\beta) via cross-validation. nmfkc.kernel.beta.nearest.med: Heuristically estimates β\\beta using median nearest-neighbor distances. nmfkc.ar.degree.cv: Selects optimal lag order (DD) NMF-VAR models.","code":""},{"path":"/index.html","id":"id_5-prediction--forecasting","dir":"","previous_headings":"","what":"5. Prediction & Forecasting","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"predict.nmfkc: Predicts values classes new covariate data (AnewA_{new}). nmfkc.ar.predict: Performs multi-step ahead forecasting Time Series models. nmfkc.ar.stationarity: Checks stationarity (stability) estimated VAR system.","code":""},{"path":"/index.html","id":"id_6-visualization-graphvizdot","dir":"","previous_headings":"","what":"6. Visualization (Graphviz/DOT)","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"Generates DOT scripts visualize relationships variables. nmfkc.DOT: Visualizes structure standard NMF Kernel NMF (→X→YA \\X \\Y). nmfkc.ar.DOT: Visualizes Granger causality network Time Series models.","code":""},{"path":"/index.html","id":"id_7-utilities","dir":"","previous_headings":"","what":"7. Utilities","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"nmfkc.normalize / nmfkc.denormalize: Helper functions scale data [0,1][0,1] back.","code":""},{"path":"/index.html","id":"statistical-model","dir":"","previous_headings":"","what":"Statistical Model","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"nmfkc package builds upon standard NMF framework incorporating external information (covariates). statistical modeling process involves three steps: Standard NMF: Let YY P×NP \\times N observation matrix. Standard Non-negative Matrix Factorization approximates YY product two non-negative matrices: Y≈XBY \\approx X B XX basis matrix (capturing latent patterns) BB coefficient matrix (weights observation). Unlike ordinary linear models XX known, NMF optimizes XX BB. NMF Covariates: assume coefficient matrix BB arbitrary driven known covariates AA (e.g., time, location, attributes). model relationship : B≈CAB \\approx C CC parameter matrix mapping covariates latent bases. Tri-Factorization Model: Substituting BB yields core model nmfkc: Y≈XCAY \\approx X C Kernel Extension: constructing AA using kernel function (e.g., Gaussian kernel) raw features, model can capture non-linear relationships (Satoh, 2024). Theoretical Roots: formulation aligns Orthogonal Tri-NMF (Ding et al., 2006) generalizes Growth Curve Models (Potthoff Roy, 1964). Structural Equation Model (NMF-SEM): applications endogenous variables affect (feedback) exogenous variables drive system, NMF-SEM models endogenous block Y1Y_1 exogenous block Y2Y_2 : Y1≈X(Θ1Y1+Θ2Y2).  Y_1 \\approx X(\\Theta_1 Y_1 + \\Theta_2 Y_2).   feedback operator XΘ1X\\Theta_1 stable, equilibrium mapping becomes: Ŷ1≈(−XΘ1)−1XΘ2Y2≡MmodelY2.  \\hat Y_1 \\approx (- X\\Theta_1)^{-1} X\\Theta_2 Y_2 \\equiv M_{model}Y_2.   provides cumulative-effect interpretation analogous Leontief input–output models, retaining NMF-style nonnegativity interpretability.","code":""},{"path":"/index.html","id":"matrices","dir":"","previous_headings":"","what":"Matrices","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"goal nmfkc optimize unknown matrices XX CC, given observation YY covariates AA, minimizing reconstruction error: Y(P,N)≈X(P,Q)×C(Q,R)×(R,N)Y(P,N) \\approx X(P,Q) \\times C(Q,R) \\times (R,N)","code":""},{"path":"/index.html","id":"given-input","dir":"","previous_headings":"","what":"Given (Input)","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"Y(P,N)Y(P,N): Observation matrix (PP features ×\\timesNN samples). Missing values supported. Can created using nmfkc.kernel (kernel methods) nmfkc.ar (time series). covariates provided, AA defaults identity matrix (Standard NMF).","code":""},{"path":"/index.html","id":"optimized-output","dir":"","previous_headings":"","what":"Optimized (Output)","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"QQ rank (number bases), constrained Q≤min(P,N)Q \\le \\min(P,N). matrix links covariates latent structure. Corresponds Θ\\Theta Satoh (2024).","code":""},{"path":"/index.html","id":"derived","dir":"","previous_headings":"","what":"Derived","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"Represents activation basis sample. Normalized columns BB (proportions) can used soft clustering probabilities.","code":""},{"path":"/index.html","id":"source","dir":"","previous_headings":"","what":"Source","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"Satoh, K. (2024) Applying Non-negative Matrix Factorization Covariates Longitudinal Data Growth Curve Model. arXiv preprint arXiv:2403.05359. https://arxiv.org/abs/2403.05359 Satoh, K. (2025) Applying non-negative Matrix Factorization Covariates Multivariate Time Series Data Vector Autoregression Model, Japanese Journal Statistics Data Science, press. https://doi.org/10.1007/s42081-025-00314-0 Satoh, K. (2025) Applying non-negative matrix factorization covariates label matrix classification. arXiv preprint arXiv:2510.10375. https://arxiv.org/abs/2510.10375","code":""},{"path":"/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"Ding, C., Li, T., Peng, W. Park, H. (2006) Orthogonal Nonnegative Matrix Tri-Factorizations Clustering, Proceedings 12th ACM SIGKDD international conference Knowledge discovery data mining, 126-135. https://doi.org/10.1145/1150402.1150420 Potthoff, R.F., Roy, S.N. (1964). generalized multivariate analysis variance model useful especially growth curve problems. Biometrika, 51, 313-326. https://doi.org/10.2307/2334137","code":""},{"path":"/index.html","id":"examples","dir":"","previous_headings":"","what":"Examples","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"practical examples demonstrating capabilities nmfkc, ranging simple matrix operations complex time-series forecasting classification tasks.","code":""},{"path":[]},{"path":"/index.html","id":"id_0-simple-matrix-operations","dir":"","previous_headings":"","what":"0. Simple matrix operations","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"example demonstrates basic concept NMF: decomposing matrix YY basis matrix XX coefficient matrix BB.","code":"# install.packages(\"remotes\") # remotes::install_github(\"ksatohds/nmfkc\")  # 1. Prepare synthetic data # X: Basis matrix (2 bases), B: Coefficient matrix (X <- cbind(c(1,0,1),c(0,1,0))) (B <- cbind(c(1,0),c(0,1),c(1,1))) (Y <- X %*% B) # Y is the observation matrix to be decomposed  # 2. Perform NMF library(nmfkc) # Decompose Y into X and B with Rank(Q) = 2 res <- nmfkc(Y, Q=2, epsilon=1e-6)  # 3. Check results res$X # Estimated Basis res$B # Estimated Coefficients  # 4. Diagnostics plot(res)    # Convergence plot of the objective function summary(res) # Summary statistics"},{"path":"/index.html","id":"id_1-longitudinal-data-covid-19-in-japan","dir":"","previous_headings":"","what":"1. Longitudinal data: COVID-19 in Japan","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"example analyzing spatiotemporal data (prefecture x time). includes Rank Selection determine optimal number clusters visualizes results map. Data Source: https://www.mhlw.go.jp/stf/covid-19/open-data.html","code":"# install.packages(\"remotes\") # remotes::install_github(\"ksatohds/nmfkc\") # install.packages(\"NipponMap\")  # 1. Data Preparation d <- read.csv(\"https://covid19.mhlw.go.jp/public/opendata/newly_confirmed_cases_daily.csv\") n <- dim(d) # Log-transform the infection counts (excluding 'Date' and 'ALL' columns) Y <- log10(1 + d[, 2 + 1:47])  rownames(Y) <- unique(d$Date)  # 2. Rank Selection Diagnostics library(nmfkc) par(mfrow=c(1,1)) # Evaluate Ranks Q=2 to 8.  # save.time=FALSE enables Element-wise Cross-Validation (Robust) result.rank <- nmfkc.rank(Y, Q=2:8, save.time=FALSE) round(result.rank$criteria, 3)  # 3. Perform NMF with Optimal Rank (e.g., Q=3) # Q <- result.rank$rank.best Q <- 3 result <- nmfkc(Y, Q=Q, epsilon=1e-5, prefix=\"Region\") plot(result, type=\"l\", col=2) # Convergence  # 4. Visualization: Individual Fit # Compare observed (black) vs fitted (red) for each prefecture par(mfrow=c(7,7), mar=c(0,0,0,0)+0.1, cex=1) for(n in 1:ncol(Y)){   plot(Y[,n], axes=FALSE, type=\"l\")    lines(result$XB[,n], col=2)    legend(\"topleft\", legend=colnames(Y)[n], x.intersp=-0.5, bty=\"n\")     box() }  # 5. Visualization: Temporal Patterns (Basis X) # Show the 3 extracted temporal patterns (Waves of infection) par(mfrow=c(Q,1), mar=c(0,0,0,0), cex=1) for(q in 1:Q){   barplot(result$X[,q], col=q+1, border=q+1, las=3,     ylim=range(result$X), ylab=paste0(\"topic \", q))    legend(\"left\", fill=q+1, legend=colnames(result$X)[q]) }  # 6. Visualization: Clustering on Map # Show which prefecture belongs to which pattern library(NipponMap) par(mfrow=c(1,1), mar=c(5,4,4,2)+0.1) jmap <- JapanPrefMap(col=\"white\", axes=TRUE)  # Soft clustering (Pie charts on map) stars(x=t(result$B.prob), scale=FALSE,       locations=jmap, key.loc=c(145,34),       draw.segments=TRUE, len=0.7, labels=NULL,       col.segments=c(1:Q)+1, add=TRUE)  # Heatmap of coefficients heatmap(t(result$B.prob))  # Hard clustering (Colored map) library(NipponMap) par(mfrow=c(1,1), mar=c(5,4,4,2)+0.1) jmap <- JapanPrefMap(col=result$B.cluster+1, axes=TRUE)"},{"path":"/index.html","id":"id_2-spatiotemporal-analysis-canadianweather","dir":"","previous_headings":"","what":"2. Spatiotemporal Analysis: CanadianWeather","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"example compares standard NMF Kernel NMF. using location information covariates (Kernel), can predict coefficients unobserved locations (Interpolation).","code":"# install.packages(\"remotes\") # remotes::install_github(\"ksatohds/nmfkc\") # install.packages(\"fda\")  library(fda) data(CanadianWeather) d <- CanadianWeather$dailyAv[,,1] # Temperature data Y <- d - min(d) # Ensure non-negative u0 <- CanadianWeather$coordinates[,2:1] # Lat/Lon u0[,1] <- -u0[,1]  # ----------------------------------- # Method A: Standard NMF (No Covariates) # ----------------------------------- library(nmfkc) result <- nmfkc(Y, Q=2, prefix=\"Trend\") plot(result)  # Visualization: Basis functions (Temperature patterns) par(mfrow=c(1,1), mar=c(5,4,2,2)+0.1, cex=1) plot(result$X[,1], type=\"n\", ylim=range(result$X), ylab=\"basis function\") Q <- ncol(result$X)   for(q in 1:Q) lines(result$X[,q], col=q+1) legend(\"topright\", legend=1:Q, fill=1:Q+1)  # Visualization: Spatial Distribution par(mfrow=c(1,1), mar=c(5,4,2,2)+0.1, cex=1) plot(u0, pch=19, cex=3, col=result$B.cluster+1, main=\"Hard Clustering\") text(u0, colnames(Y), pos=1)  # ----------------------------------- # Method B: NMF with Covariates (Linear) # ----------------------------------- # Using raw coordinates as covariates A U <- t(nmfkc.normalize(u0))  A <- rbind(1, U) # Add intercept result <- nmfkc(Y, A, Q=2, prefix=\"Trend\") result$r.squared  # ----------------------------------- # Method C: NMF with Covariates (Gaussian Kernel) # ----------------------------------- # A is constructed using a Kernel function of locations U. # K(u,v) = exp{-beta * |u-v|^2}  # 1. Optimize Gaussian kernel parameter (beta) via Cross-Validation result.beta <- nmfkc.kernel.beta.cv(Y, Q=2, U=U, beta=c(0.5, 1, 2, 5, 10)) (best.beta <- result.beta$beta)  # 2. Perform Kernel NMF A <- nmfkc.kernel(U, beta=best.beta) result <- nmfkc(Y, A, Q=2, prefix=\"Trend\") result$r.squared   # 3. Prediction / Interpolation on a Mesh # Predict coefficients B for new locations V v <- seq(from=0, to=1, length=20) V <- t(cbind(expand.grid(v,v))) # Grid points A_new <- nmfkc.kernel(U, V, beta=best.beta)  # B_new = C * A_new B <- result$C %*% A_new B.prob <- prop.table(B, 2)  # Visualize estimated probability surface q <- 1 z <- matrix(B.prob[q,], nrow=length(v))  par(mfrow=c(1,1), mar=c(5,4,2,2)+0.1, cex=1) filled.contour(v, v, z, main=paste0(\"Predicted Probability of Pattern \",q),   color.palette = function(n) hcl.colors(n, \"Greens3\", rev=TRUE),   plot.axes={      points(t(U), col=7, pch=19)      text(t(U), colnames(U), pos=3)   } )"},{"path":"/index.html","id":"id_3-topic-model-us-presidential-inaugural-addresses","dir":"","previous_headings":"","what":"3. Topic model: US Presidential Inaugural Addresses","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"Application text mining. NMF extracts topics (XX) weights (BB). using “Year” covariate, can visualize topics evolve time.","code":"# install.packages(\"remotes\") # remotes::install_github(\"ksatohds/nmfkc\") # install.packages(\"quanteda\")  # 1. Text Preprocessing using quanteda library(quanteda) corp <- corpus(data_corpus_inaugural) tok <- tokens(corp) tok <- tokens_remove(tok, pattern=stopwords(\"en\", source=\"snowball\")) df <- dfm(tok) df <- dfm_select(df, min_nchar=3) df <- dfm_trim(df, min_termfreq=100) d <- as.matrix(df) # Sort by frequency index <- order(colSums(d), decreasing=TRUE)  d <- d[,index]   # ----------------------------------- # Standard Topic Model (NMF) # ----------------------------------- Y <- t(d) # Term-Document Matrix Q <- 3 library(nmfkc) result <- nmfkc(Y, Q=Q, prefix=\"Topic\")  # Interpret Topics (High probability words) Xp <- result$X.prob for(q in 1:Q){   message(paste0(\"----- Featured words on Topic [\", q, \"] -----\"))   print(paste0(rownames(Xp), \"(\", rowSums(Y), \") \", round(100*Xp[,q], 1), \"%\")[Xp[,q]>=0.5]) }  # Visualize Topic Proportions per President par(mfrow=c(1,1), mar=c(10,4,4,2)+0.1, cex=1) barplot(result$B.prob, col=1:Q+1, legend=TRUE, las=3,   ylab=\"Probabilities of topics\")  # ----------------------------------- # Temporal Topic Model (Kernel NMF) # ----------------------------------- # Use 'Year' as covariate to smooth topic trends U <- t(as.matrix(corp$Year))  # Optimize beta result.beta <- nmfkc.kernel.beta.cv(Y, Q=3, U, beta=c(0.2, 0.5, 1, 2, 5)/10000) (best.beta <- result.beta$beta)  # Perform Kernel NMF A <- nmfkc.kernel(U, beta=best.beta) result <- nmfkc(Y, A, Q, prefix=\"Topic\")  # Visualize Smooth Topic Evolution colnames(result$B.prob) <- corp$Year par(mfrow=c(1,1), mar=c(5,4,2,2)+0.1, cex=1) barplot(result$B.prob, col=1:Q+1, legend=TRUE, las=3,          ylab=\"Probability of topic (Smoothed)\")"},{"path":"/index.html","id":"id_4-origin-destination-od-data","dir":"","previous_headings":"","what":"4. Origin-Destination (OD) data","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"Analyzing flow data prefectures. example uses Sankey diagrams visualize stability clustering results across different ranks (QQ).","code":"# install.packages(\"remotes\") # remotes::install_github(\"ksatohds/nmfkc\") # install.packages(\"httr\") # install.packages(\"readxl\") # install.packages(\"alluvial\") # install.packages(\"NipponMap\") # install.packages(\"RColorBrewer\")  # 1. Data Download & Formatting (Japanese OD data) library(httr) url <- \"https://www.e-stat.go.jp/stat-search/file-download?statInfId=000040170612&fileKind=0\" GET(url, write_disk(tmp <- tempfile(fileext=\".xlsx\"))) library(readxl) d <- as.data.frame(read_xlsx(tmp, sheet=1, skip=2)) # Filter for specific flow type d <- d[d[,1]==\"1\" & d[,3]==\"02\" & d[,5]!=\"100\" & d[,7]!=\"100\", ] pref <- unique(d[,6])  # Create OD Matrix Y (47x47) Y <- matrix(NA, nrow=47, ncol=47) colnames(Y) <- rownames(Y) <- pref d[,5] <- as.numeric(d[,5]); d[,7] <- as.numeric(d[,7]) for(i in 1:47) for(j in 1:47) Y[i,j] <- d[which(d[,5]==i & d[,7]==j), 9] Y <- log(1 + Y) # Log transformation  # 2. Rank Selection & Diagnostic Plot library(nmfkc) # Check AIC, BIC, RSS for Q=2 to 12 nmfkc.rank(Y, Q=2:12, save.time=FALSE)  # 3. NMF Analysis (Q=7) Q0 <- 7 res <- nmfkc(Y, Q=Q0, save.time=FALSE, prefix=\"Region\")  # 4. Visualization: Silhouette Plot si <- res$criterion$silhouette barplot(si$silhouette, horiz=TRUE, las=1, col=si$cluster+1,          cex.names=0.5, xlab=\"Silhouette width\") abline(v=si$silhouette.mean, lty=3)  # 5. Visualization: Sankey Diagram (Stability Analysis) # Visualize how clusters change as Q increases from 6 to 8 Q <- 6:8 cluster <- NULL for(i in 1:length(Q)){   res_tmp <- nmfkc(Y, Q=Q[i])   cluster <- cbind(cluster, res_tmp$B.cluster) } library(alluvial) alluvial(cluster, freq=1, axis_labels=paste0(\"Q=\", Q), cex=2,          col=cluster[,2]+1, border=cluster[,2]+1) title(\"Cluster evolution (Sankey Diagram)\")  # 6. Visualization: Map library(NipponMap); library(RColorBrewer) mypalette <- brewer.pal(12, \"Paired\") par(mfrow=c(1,1), mar=c(5,4,4,2)+0.1) jmap <- JapanPrefMap(col=mypalette[res$B.cluster], axes=TRUE) text(jmap, pref, cex=0.5)   title(main=\"OD Clustering Result\")"},{"path":"/index.html","id":"id_5-kernel-ridge-regression","dir":"","previous_headings":"","what":"5. Kernel ridge regression","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"Demonstrates nmfkc can used non-linear regression (smoothing) treating 1D data matrix row using Gaussian kernels.","code":"# install.packages(\"remotes\") # remotes::install_github(\"ksatohds/nmfkc\")  library(MASS) d <- mcycle x <- d$times y <- d$accel # Treat Y as a 1 x N matrix Y <- t(as.matrix(y - min(y))) U <- t(as.matrix(x))  # 1. Linear Regression (NMF with Linear Covariate) A <- rbind(1, U) # Intercept + Linear term library(nmfkc) result <- nmfkc(Y, A, Q=1) par(mfrow=c(1,1), mar=c(5,4,2,2)+0.1, cex=1) plot(U, Y, main=\"Linear Fit\") lines(U, result$XB, col=2)  # 2. Kernel Regression (Gaussian Kernel) # Optimize beta via Cross-Validation result.beta <- nmfkc.kernel.beta.cv(Y, Q=1, U, beta=2:5/100) (beta.best <- result.beta$beta)    # Create Kernel Matrix A <- nmfkc.kernel(U, beta=beta.best) result <- nmfkc(Y, A, Q=1)  # Plot Fitted Curve (Smoothing) plot(U, Y, main=\"Kernel Smoothing\") lines(U, as.vector(result$XB), col=2, lwd=2)  # Prediction on new data points V <- matrix(seq(from=min(U), to=max(U), length=100), ncol=100) A_new <- nmfkc.kernel(U, V, beta=beta.best) XB_new <- predict(result, newA=A_new) lines(V, as.vector(XB_new), col=4, lwd=2)"},{"path":"/index.html","id":"id_6-growth-curve-model","dir":"","previous_headings":"","what":"6. Growth curve model","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"Analysis repeated measures (Growth Curve). Shows incorporate categorical covariates (Sex) AA matrix analyze group differences.","code":"# install.packages(\"remotes\") # remotes::install_github(\"ksatohds/nmfkc\") # install.packages(\"nlme\")  library(nlme) d <- Orthodont t <- unique(d$age) # Matrix: Age x Subject Y <- matrix(d$distance, nrow=length(t)) colnames(Y) <- unique(d$Subject) rownames(Y) <- t  # 1. Construct Covariate Matrix A # Include Intercept and Dummy variable for Male Male <- 1 * (d$Sex == \"Male\")[d$age == 8] A <- rbind(rep(1, ncol(Y)), Male) rownames(A) <- c(\"Const\", \"Male\")  # 2. Perform NMF with Covariates library(nmfkc) result <- nmfkc(Y, A, Q=2, epsilon=1e-8)  # 3. Check Coefficients # B = C * A. We can see the coefficients for Male vs Female. (A0 <- t(unique(t(A)))) # Unique patterns (Female, Male) B <- result$C %*% A0  # 4. Visualization plot(t, Y[,1], ylim=range(Y), type=\"n\", ylab=\"Distance\", xlab=\"Age\") mycol <- ifelse(Male==1, 4, 2) for(n in 1:ncol(Y)){   lines(t, Y[,n], col=mycol[n]) # Individual trajectories } XB <- result$X %*% B # Mean trajectories lines(t, XB[,1], col=4, lwd=5) # Male lines(t, XB[,2], col=2, lwd=5) # Female legend(\"topleft\", title=\"Sex\", legend=c(\"Male\", \"Female\"), fill=c(4,2))"},{"path":"/index.html","id":"id_7-autoregression-airpassengers-nmf-var","dir":"","previous_headings":"","what":"7. Autoregression: AirPassengers (NMF-VAR)","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"nmfkc can perform Vector Autoregression (NMF-VAR). example includes Lag Order Selection Future Forecasting.","code":"library(nmfkc) data(AirPassengers) d <- log10(AirPassengers) is.ts(d)  # --- 1. Lag Order Selection --- # Perform cross-validation to select the optimal lag order (degree). # We evaluate degrees 1 to 15 to capture potential seasonality (e.g., lag 12). nmfkc.ar.degree.cv(Y=d, degree=1:15, epsilon=1e-5, maxit=50000)  # --- 2. Model Construction (NMF-VAR) --- # Construct the observation matrix (Y) and covariate matrix (A) for NMF-VAR. # We use degree=12 to account for the annual seasonality of the monthly data. # 'intercept=TRUE' adds an intercept term to the covariate matrix. a <- nmfkc.ar(d, degree = 12, intercept = TRUE) Y <- a$Y A <- a$A  # --- 3. Model Fitting --- # Fit the NMF-VAR model (Y approx X * C * A). # We use Rank (Q) = 1 for this univariate time series. # Stricter convergence criteria (epsilon, maxit) are used for precision. res <- nmfkc(Y=Y, A=A, Q=1, epsilon=1e-6, maxit=50000)  # Display the summary of the fitted model (R-squared, sparsity, etc.). summary(res)  # --- 4. Forecasting --- # Compute multi-step-ahead forecasts (recursive forecasting). # We predict for the next 24 time points (2 years). pred_res <- nmfkc.ar.predict(res, Y = Y, n.ahead = 24)  # --- 5. Data Preparation for Plotting --- # Extract time points from column names (automatically handled by nmfkc.ar). time_obs <- as.numeric(colnames(Y))  # Back-transform values from log10 scale to original scale. vals_obs <- 10^as.vector(Y)        # Observed values vals_fit <- 10^as.vector(res$XB)   # Fitted values (Learning phase)  # Prepare forecast data (time and values). time_pred <- as.numeric(colnames(pred_res$pred)) vals_pred <- 10^as.vector(pred_res$pred)  # Determine plot limits to include both observed and predicted data. xlim <- range(c(time_obs, time_pred)) ylim <- range(c(vals_obs, vals_pred))  # --- 6. Visualization --- # Plot the observed data (Gray line). plot(time_obs, vals_obs, type = \"l\", col = \"gray\", lwd = 2,      xlim = xlim, ylim = ylim,      main = \"AirPassengers: NMF-VAR Forecast\",      xlab = \"Year\", ylab = \"Passengers (Thousands)\")  # Add the fitted values (Blue line). lines(time_obs, vals_fit, col = \"blue\", lwd = 1.5)  # Add the forecast values (Red line). lines(time_pred, vals_pred, col = \"red\", lwd = 2, lty = 1)  # Add a vertical line indicating the start of the forecast period. abline(v = min(time_pred), col = \"black\", lty = 3)  # Add a legend. legend(\"topleft\", legend = c(\"Observed\", \"Fitted\", \"Forecast\"),        col = c(\"gray\", \"blue\", \"red\"), lty = c(1, 1, 1), lwd = 2)"},{"path":"/index.html","id":"id_8-vector-autoregression-canada-multivariate","dir":"","previous_headings":"","what":"8. Vector Autoregression: Canada (Multivariate)","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"Multivariate NMF-VAR example. also demonstrates visualize Granger causality using DOT graphs.","code":"library(nmfkc) library(vars)  # 1. Data Preparation (Keep 'ts' class!) d0 <- Canada  # ts object  # Difference and Normalize # apply() strips 'ts' class, so we must restore it to use nmfkc.ar's feature dd_mat <- apply(d0, 2, diff) dd_ts  <- ts(dd_mat, start = time(d0)[2], frequency = frequency(d0))  # Normalize (returns matrix, so restore 'ts' again) dn_mat <- nmfkc.normalize(dd_ts) dn_ts  <- ts(dn_mat, start = start(dd_ts), frequency = frequency(dd_ts))  # 2. NMF-VAR Matrix Construction # [Point] Pass the 'ts' object (Time x Var) DIRECTLY! # nmfkc.ar will detect it, transpose it to (Var x Time), and preserve time info. Q <- 2 D <- 1 ar_set <- nmfkc.ar(dn_ts, degree = D, intercept = TRUE)  Y <- ar_set$Y # Has time-based colnames! A <- ar_set$A # Has time-based colnames!  # 3. Fit Model res <- nmfkc(Y = Y, A = A, Q = Q, prefix = \"Condition\", epsilon = 1e-6)  # 4. Visualization (Using preserved time info) # No need to manually reconstruct 'ts' objects or calculate time axes. # The colnames of Y and res$XB are already \"1980.25\", \"1980.50\", etc.  time_vec <- as.numeric(colnames(Y)) # Extract time from colnames  # Plot Condition 1 (e.g., Employment/Productivity factor) # res$B.prob contains soft clustering probabilities par(mfrow = c(2, 1), mar = c(4, 4, 2, 1)) barplot(res$B.prob,          col = c(\"tomato\", \"turquoise\"),          border = NA,          names.arg = time_vec, # Use time directly!         las = 2, cex.names = 0.6,         main = \"Economic Conditions (Soft Clustering)\",         ylab = \"Probability\") legend(\"bottomright\", legend = rownames(res$B.prob),         fill = c(\"tomato\", \"turquoise\"), bty = \"n\")  # Plot Fitted Values for a variable (e.g., Employment 'e') # Row 1 of Y corresponds to variable 'e' plot(time_vec, Y[1, ], type = \"l\", col = \"gray\", lwd = 2,      main = \"Employment (Normalized Diff)\", xlab = \"Year\", ylab = \"Value\") lines(time_vec, res$XB[1, ], col = \"red\", lwd = 1.5) legend(\"topleft\", legend = c(\"Observed\", \"Fitted\"),         col = c(\"gray\", \"red\"), lwd = 2, bty = \"n\")"},{"path":"/index.html","id":"id_9-classification-iris-nmf-lab","dir":"","previous_headings":"","what":"9. Classification: Iris (NMF-LAB)","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"NMF-LAB (Label-based NMF) example. treating class labels target matrix YY (one-hot encoding) features covariates UU (kernelized AA), NMF can used Supervised Learning.","code":"# install.packages(\"remotes\") # remotes::install_github(\"ksatohds/nmfkc\") library(nmfkc)  # 1. Data Preparation label <- iris$Species # Convert labels to One-Hot Matrix Y Y <- nmfkc.class(label)  # Features Matrix U (Normalized) U <- t(nmfkc.normalize(iris[,-5]))  # 2. Kernel Parameter Optimization # Heuristic estimation of beta (Median distance) res.beta <- nmfkc.kernel.beta.nearest.med(U) # Cross-validation for fine-tuning res.cv <- nmfkc.kernel.beta.cv(Y, Q=length(unique(label)), U, beta=res.beta$beta_candidates) best.beta <- res.cv$beta  # 3. Fit NMF-LAB Model A <- nmfkc.kernel(U, beta=best.beta) res <- nmfkc(Y=Y, A=A, Q=length(unique(label)), prefix=\"Class\")  # 4. Prediction and Evaluation # Predict class based on highest probability in XB fitted.label <- predict(res, type=\"class\") (f <- table(fitted.label, label)) message(\"Accuracy: \", round(100 * sum(diag(f)) / sum(f), 2), \"%\")"},{"path":"/index.html","id":"id_10-structural-equation-modeling-holzingerswineford1939-nmf-sem","dir":"","previous_headings":"","what":"10. Structural equation modeling: HolzingerSwineford1939 (NMF-SEM)","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"example demonstrates NMF-SEM, nonnegative structural equation model endogenous feedback equilibrium mapping. split variables endogenous block Y1Y_1 (test scores) exogenous block Y2Y_2 (demographics), fit: Baseline (feedback): Y1≈XCY2Y_1 \\approx X C Y_2 NMF-SEM (feedback): Y1≈X(Θ1Y1+Θ2Y2)Y_1 \\approx X(\\Theta_1 Y_1 + \\Theta_2 Y_2)","code":"library(lavaan) library(nmfkc)  data(HolzingerSwineford1939) d <- HolzingerSwineford1939 d <- d[complete.cases(d), ]  # Exogenous variables (demographics) d$age.rev <- -(d$ageyr + d$agemo/12) d$sex.2 <- ifelse(d$sex == 2, 1, 0) d$school.GW <- ifelse(d$school == \"Grant-White\", 1, 0) d[, c(\"id\",\"sex\",\"ageyr\",\"agemo\",\"school\",\"grade\")] <- NULL  # Nonnegative normalization d <- nmfkc.normalize(d)  exogenous_vars <- c(\"age.rev\", \"sex.2\", \"school.GW\") endogenous_vars <- setdiff(colnames(d), exogenous_vars)  Y1 <- t(d[, endogenous_vars]) Y2 <- t(d[, exogenous_vars])  # Baseline mapping: Y1 ≈ X C Y2 Q0 <- 3 res0 <- nmfkc(Y = Y1, A = Y2, Q = Q0, epsilon = 1e-6, X.L2.ortho = 100) M.simple <- res0$X %*% res0$C  # NMF-SEM: Y1 ≈ X(C1 Y1 + C2 Y2) res <- nmf.sem(   Y1, Y2,   rank = Q0,   X.init = res0$X,   X.L2.ortho = 100,   C1.L1 = 10,   C2.L1 = 0.6,   epsilon = 1e-6 )  # Diagnostics SC.map <- cor(as.vector(res$M.model), as.vector(M.simple)) cat(\"rho(XC1)=\", round(res$XC1.radius, 3), \"\\n\") cat(\"AR=\", round(res$amplification, 3), \"\\n\") cat(\"SCmap=\", round(SC.map, 3), \"\\n\") cat(\"SCcov=\", round(res$SC.cov, 3), \"\\n\") cat(\"MAE=\", round(res$MAE, 3), \"\\n\")  # Graph visualization (DOT) res.dot <- nmf.sem.DOT(res, weight_scale = 5, rankdir = \"TB\",                        threshold = 0.01, fill = FALSE,                        cluster.box = \"none\") library(DiagrammeR) grViz(res.dot)"},{"path":"/index.html","id":"author","dir":"","previous_headings":"","what":"Author","title":"Non-Negative Matrix Factorization with Kernel Covariates","text":"Kenichi Satoh, homepage","code":""},{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2023 Kenichi Satoh Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/reference/nmf.sem.cv.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-Validation for NMF-SEM — nmf.sem.cv","title":"Cross-Validation for NMF-SEM — nmf.sem.cv","text":"Performs K-fold cross-validation evaluate equilibrium mapping NMF-SEM model. fold, nmf.sem fitted training samples, yielding equilibrium mapping \\(\\hat Y_1 = M_{\\mathrm{model}} Y_2\\). held-endogenous variables \\(Y_1\\) predicted \\(Y_2\\) using mapping, mean absolute error (MAE) entries test block computed. returned value average MAE across folds. implements hyperparameter selection strategy described paper: hyperparameters chosen predictive cross-validation rather direct inspection internal structural matrices.","code":""},{"path":"/reference/nmf.sem.cv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-Validation for NMF-SEM — nmf.sem.cv","text":"","code":"nmf.sem.cv(   Y1,   Y2,   rank = NULL,   X.init = NULL,   X.L2.ortho = 100,   C1.L1 = 0.5,   C2.L1 = 0,   epsilon = 1e-04,   maxit = 50000,   seed = NULL,   div = 5,   shuffle = TRUE,   ... )"},{"path":"/reference/nmf.sem.cv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-Validation for NMF-SEM — nmf.sem.cv","text":"Y1 non-negative numeric matrix endogenous variables rows = variables (P1), columns = samples (N). Y2 non-negative numeric matrix exogenous variables rows = variables (P2), columns = samples (N). Must satisfy ncol(Y1) == ncol(Y2). rank Integer; rank (number latent factors) passed nmf.sem. NULL, nmf.sem decides effective rank (via ... nrow(Y2)). X.init Optional initialization X (nmf.sem). X.L2.ortho L2 orthogonality penalty X. C1.L1 L1 sparsity penalty C1 (\\(\\Theta_1\\)). C2.L1 L1 sparsity penalty C2 (\\(\\Theta_2\\)). epsilon Convergence threshold nmf.sem. maxit Maximum number iterations nmf.sem. seed Master random seed CV splitting fold-specific calls nmf.sem. NULL, RNG controlled within folds. div Number CV folds. (Default: 5) shuffle Logical; TRUE, samples randomly permuted assigning folds. (Default: TRUE) ... Additional arguments passed nmf.sem (except rank, seed, div, shuffle, handled ).","code":""},{"path":"/reference/nmf.sem.cv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-Validation for NMF-SEM — nmf.sem.cv","text":"numeric scalar: mean MAE across CV folds.","code":""},{"path":"/reference/nmf.sem.DOT.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a Graphviz DOT Diagram for an NMF-SEM Model — nmf.sem.DOT","title":"Generate a Graphviz DOT Diagram for an NMF-SEM Model — nmf.sem.DOT","text":"Creates Graphviz DOT script visualizes structural network estimated nmf.sem. resulting diagram displays: endogenous observed variables (\\(Y_1\\)), exogenous observed variables (\\(Y_2\\)), latent factors (\\(F_1\\), ..., \\(F_Q\\)), together non-negative path coefficients whose magnitudes exceed user-specified threshold. Directed edges represent estimated relationships: \\(Y_2 \\rightarrow F_q\\): entries C2 (exogenous loadings), \\(F_q \\rightarrow Y_1\\): rows X (factor--endogenous mappings), \\(Y_1 \\rightarrow F_q\\): entries C1 (feedback paths). Edge widths scaled coefficient magnitude, nodes placed optional visual clusters. variables participating edges threshold displayed, latent factors always shown.","code":""},{"path":"/reference/nmf.sem.DOT.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a Graphviz DOT Diagram for an NMF-SEM Model — nmf.sem.DOT","text":"","code":"nmf.sem.DOT(   result,   weight_scale = 5,   weight_scale_y2f = weight_scale,   weight_scale_fy1 = weight_scale,   weight_scale_feedback = weight_scale,   threshold = 0.01,   rankdir = \"LR\",   fill = TRUE,   cluster.box = c(\"normal\", \"faint\", \"invisible\", \"none\"),   cluster.labels = NULL )"},{"path":"/reference/nmf.sem.DOT.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a Graphviz DOT Diagram for an NMF-SEM Model — nmf.sem.DOT","text":"result list returned nmf.sem, containing matrices X, C1, C2. weight_scale Base scaling factor edge widths. weight_scale_y2f Optional override scaling edges \\(Y_2 \\rightarrow F_q\\). Defaults weight_scale. weight_scale_fy1 Optional override scaling edges \\(F_q \\rightarrow Y_1\\). Defaults weight_scale. weight_scale_feedback Optional override scaling feedback edges \\(Y_1 \\rightarrow F_q\\). Defaults weight_scale. threshold Minimum coefficient value needed edge drawn. rankdir Graphviz rank direction (e.g., \"LR\", \"TB\"). fill Logical; whether use filled node shapes. cluster.box Character string controlling visibility style cluster frames around Y2, factors, Y1 blocks. One \"normal\", \"faint\", \"invisible\", \"none\". cluster.labels Optional character vector length 3 giving custom labels Y2, factor, Y1 clusters.","code":""},{"path":"/reference/nmf.sem.DOT.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a Graphviz DOT Diagram for an NMF-SEM Model — nmf.sem.DOT","text":"character string representing valid Graphviz DOT script.","code":""},{"path":"/reference/nmf.sem.html","id":null,"dir":"Reference","previous_headings":"","what":"NMF-SEM Main Estimation Algorithm — nmf.sem","title":"NMF-SEM Main Estimation Algorithm — nmf.sem","text":"Fits NMF-SEM model $$   Y_1 \\approx X \\bigl( \\Theta_1 Y_1 + \\Theta_2 Y_2 \\bigr) $$ non-negativity constraints orthogonality sparsity regularization. function returns estimated latent factors, structural coefficient matrices, implied equilibrium (input–output) mapping. equilibrium, model can written $$   Y_1 \\approx (- X \\Theta_1)^{-1} X \\Theta_2 Y_2   \\equiv M_{\\mathrm{model}} Y_2, $$ \\(M_{\\mathrm{model}} = (- X \\Theta_1)^{-1} X \\Theta_2\\) Leontief-type cumulative-effect operator latent space. Internally, latent feedback exogenous loading matrices stored C1 C2, corresponding \\(\\Theta_1\\) \\(\\Theta_2\\), respectively.","code":""},{"path":"/reference/nmf.sem.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"NMF-SEM Main Estimation Algorithm — nmf.sem","text":"","code":"nmf.sem(   Y1,   Y2,   rank = NULL,   X.init = NULL,   X.L2.ortho = 100,   C1.L1 = 1,   C2.L1 = 0.1,   epsilon = 1e-06,   maxit = 20000,   seed = 123,   ... )"},{"path":"/reference/nmf.sem.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"NMF-SEM Main Estimation Algorithm — nmf.sem","text":"Y1 non-negative numeric matrix endogenous variables rows = variables (P1), columns = samples (N). Y2 non-negative numeric matrix exogenous variables rows = variables (P2), columns = samples (N). Must satisfy ncol(Y1) == ncol(Y2). rank Integer; number latent factors \\(Q\\). NULL, \\(Q\\) taken hidden argument ... defaults nrow(Y2). X.init Optional non-negative initialization basis matrix X (\\(P_1 \\times Q\\)). supplied, projected non-negative column-normalized. X.L2.ortho L2 orthogonality penalty X. controls penalty term \\(\\lambda_X \\lVert X^\\top X - \\mathrm{diag}(X^\\top X)   \\rVert_F^2\\). Default: 100. C1.L1 L1 sparsity penalty C1 (.e., \\(\\Theta_1\\)). Default: 1.0. C2.L1 L1 sparsity penalty C2 (.e., \\(\\Theta_2\\)). Default: 0.1. epsilon Relative convergence threshold objective function. Iterations stop relative change reconstruction loss falls value. Default: 1e-6. maxit Maximum number iterations multiplicative updates. Default: 20000. seed Random seed used initialize X, C1, C2. Default: 123. ... Additional arguments. Currently used pass hidden rank Q (e.g., via Q = 3) rank NULL.","code":""},{"path":"/reference/nmf.sem.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"NMF-SEM Main Estimation Algorithm — nmf.sem","text":"list components: X Estimated basis matrix (\\(P_1 \\times Q\\)). C1 Estimated latent feedback matrix (\\(\\Theta_1\\), \\(Q \\times P_1\\)). C2 Estimated exogenous loading matrix (\\(\\Theta_2\\), \\(Q \\times P_2\\)). XC1 Feedback matrix \\(X \\Theta_1\\). XC2 Direct-effect matrix \\(X \\Theta_2\\). XC1.radius Spectral radius \\(\\rho(X \\Theta_1)\\). XC1.norm1 Induced 1-norm \\(\\lVert X \\Theta_1 \\rVert_{1,\\mathrm{op}}\\). Leontief.inv Leontief-type inverse \\((- X \\Theta_1)^{-1}.\\) M.model Equilibrium mapping \\(M_{\\mathrm{model}} = (- X \\Theta_1)^{-1} X \\Theta_2\\). amplification Latent amplification factor \\(\\lVert M_{\\mathrm{model}} \\rVert_{1,\\mathrm{op}} /          \\bigl\\lVert X \\Theta_2 \\bigr\\rVert_{1,\\mathrm{op}}\\). amplification.bound Geometric-series upper bound \\(1 / (1 - \\lVert X \\Theta_1 \\rVert_{1,\\mathrm{op}})\\) \\(\\lVert X \\Theta_1 \\rVert_{1,\\mathrm{op}} < 1\\), otherwise Inf. Q Effective latent dimension used fit. SC.cov Correlation sample model-implied covariance (flattened) \\(Y_1\\). MAE Mean absolute error \\(Y_1\\) equilibrium prediction \\(\\hat Y_1 = M_{\\mathrm{model}} Y_2\\). objfunc Vector reconstruction losses per iteration. objfunc.full Vector penalized objective values per iteration. iter Number iterations actually performed.","code":""},{"path":"/reference/nmf.sem.split.html","id":null,"dir":"Reference","previous_headings":"","what":"Heuristic Variable Splitting for NMF-SEM — nmf.sem.split","title":"Heuristic Variable Splitting for NMF-SEM — nmf.sem.split","text":"Infers heuristic partition observed variables exogenous (\\(Y_2\\)) endogenous (\\(Y_1\\)) blocks use NMF-SEM. method based positive-SEM logic, causal ordering, optional sign alignment using first principal component (PC1). procedure: internally standardizes variables (mean 0, sd 1), optionally flips signs variables align positively PC1, infers causal ordering repeatedly regressing variable remaining ones selecting variable largest minimum standardized coefficient, determines exogenous block scanning ordering upstream stopping first variable whose strongest parent coefficient exceeds threshold. n.exogenous supplied, overrides automatic threshold rule.","code":""},{"path":"/reference/nmf.sem.split.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Heuristic Variable Splitting for NMF-SEM — nmf.sem.split","text":"","code":"nmf.sem.split(   x,   n.exogenous = NULL,   threshold = 0.1,   auto.flipped = TRUE,   verbose = TRUE )"},{"path":"/reference/nmf.sem.split.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Heuristic Variable Splitting for NMF-SEM — nmf.sem.split","text":"x numeric matrix data frame rows = samples columns = observed variables. n.exogenous Optional integer specifying number exogenous variables (\\(Y_2\\)). NULL, number inferred automatically coefficient cut-rule. threshold Standardized regression-coefficient threshold used automatic exogenous–endogenous split. variable treated endogenous maximum standardized parent coefficient exceeds value. (Default: 0.1) auto.flipped Logical; TRUE, applies PC1-based automatic sign flipping standardization ensure consistent orientation. (Default: TRUE) verbose Logical; TRUE, prints progress messages resulting variable split. (Default: TRUE)","code":""},{"path":"/reference/nmf.sem.split.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Heuristic Variable Splitting for NMF-SEM — nmf.sem.split","text":"list : endogenous.variables Character vector variables selected endogenous (\\(Y_1\\)). exogenous.variables Character vector variables selected exogenous (\\(Y_2\\)). ordered.variables Variables inferred causal order (exogenous endogenous). .flipped Logical vector indicating variables sign-flipped processing. n.exogenous Integer giving number exogenous variables.","code":""},{"path":"/reference/nmfkc.ar.degree.cv.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimize lag order for the autoregressive model — nmfkc.ar.degree.cv","title":"Optimize lag order for the autoregressive model — nmfkc.ar.degree.cv","text":"nmfkc.ar.degree.cv selects optimal lag order autoregressive model applying cross-validation candidate degrees. function accepts standard matrices (Variables x Time) ts objects (Time x Variables). ts objects automatically transposed internally.","code":""},{"path":"/reference/nmfkc.ar.degree.cv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimize lag order for the autoregressive model — nmfkc.ar.degree.cv","text":"","code":"nmfkc.ar.degree.cv(Y, Q = 1, degree = 1:2, intercept = TRUE, plot = TRUE, ...)"},{"path":"/reference/nmfkc.ar.degree.cv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimize lag order for the autoregressive model — nmfkc.ar.degree.cv","text":"Y Observation matrix \\(Y(P,N)\\) ts object. Q Rank basis matrix. Must satisfy \\(Q \\le \\min(P,N)\\). degree vector candidate lag orders evaluated. intercept Logical. TRUE (default), intercept added covariate matrix. plot Logical. TRUE (default), plot objective function values drawn. ... Additional arguments passed nmfkc.cv.","code":""},{"path":"/reference/nmfkc.ar.degree.cv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimize lag order for the autoregressive model — nmfkc.ar.degree.cv","text":"list components: degree lag order minimizes cross-validation objective function. degree.max Maximum recommended lag order, computed \\(10 \\log_{10}(N)\\) following ar function stats package. objfunc Objective function values candidate lag order.","code":""},{"path":[]},{"path":"/reference/nmfkc.ar.degree.cv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimize lag order for the autoregressive model — nmfkc.ar.degree.cv","text":"","code":"# install.packages(\"remotes\") # remotes::install_github(\"ksatohds/nmfkc\") # Example using ts object directly d <- AirPassengers  # Selection of degree (using ts object) # Note: Y is automatically transposed if it is a ts object nmfkc.ar.degree.cv(Y=d, Q=1, degree=11:14) #> degree=11... #> 0.1sec #> degree=12... #> 0.1sec #> degree=13... #> 0.1sec #> degree=14... #> 0.1sec  #> $degree #> [1] 12 #>  #> $degree.max #> [1] 21 #>  #> $objfunc #>       11       12       13       14  #> 507.7053 271.4061 276.0141 278.1065  #>"},{"path":"/reference/nmfkc.ar.DOT.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a Graphviz DOT Diagram for NMF-AR / NMF-VAR Models — nmfkc.ar.DOT","title":"Generate a Graphviz DOT Diagram for NMF-AR / NMF-VAR Models — nmfkc.ar.DOT","text":"Produces Graphviz DOT script visualizing autoregressive NMF--covariates models constructed via nmfkc.ar + nmfkc. diagram displays three types directed relationships: Lagged predictors: \\(T_{t-k} \\rightarrow X\\), Current latent factors: \\(X \\rightarrow T_t\\), Optional intercept effects: Const -> X. Importantly, direct edges lagged variables current outputs (\\(T_{t-k} \\rightarrow T_t\\)) drawn, accordance NMF-AR formulation. block lagged variables displayed DOT subgraph (e.g., “T-1”, “T-2”, ...), latent factor nodes current-time outputs arranged separate clusters.","code":""},{"path":"/reference/nmfkc.ar.DOT.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a Graphviz DOT Diagram for NMF-AR / NMF-VAR Models — nmfkc.ar.DOT","text":"","code":"nmfkc.ar.DOT(   x,   degree = 1,   intercept = FALSE,   threshold = 0.1,   rankdir = \"RL\",   fill = TRUE,   weight_scale_xy = 5,   weight_scale_lag = 5,   weight_scale_int = 3 )"},{"path":"/reference/nmfkc.ar.DOT.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a Graphviz DOT Diagram for NMF-AR / NMF-VAR Models — nmfkc.ar.DOT","text":"x fitted nmfkc object representing AR model. Must contain matrices X C. degree Maximum AR lag visualize. intercept Logical; TRUE, draws intercept nodes columns named \"(Intercept)\" matrix C. threshold Minimum coefficient magnitude required draw edge. rankdir Graphviz rank direction (e.g., \"RL\", \"LR\", \"TB\"). fill Logical; whether nodes filled color. weight_scale_xy Scaling factor edges \\(X \\rightarrow T\\). weight_scale_lag Scaling factor lagged edges \\(T-k \\rightarrow X\\). weight_scale_int Scaling factor intercept edges.","code":""},{"path":"/reference/nmfkc.ar.DOT.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a Graphviz DOT Diagram for NMF-AR / NMF-VAR Models — nmfkc.ar.DOT","text":"character string representing Graphviz DOT file.","code":""},{"path":"/reference/nmfkc.ar.html","id":null,"dir":"Reference","previous_headings":"","what":"Construct observation and covariate matrices for a vector autoregressive model — nmfkc.ar","title":"Construct observation and covariate matrices for a vector autoregressive model — nmfkc.ar","text":"nmfkc.ar generates observation matrix covariate matrix corresponding specified autoregressive lag order. input Y ts object, time properties preserved \"tsp_info\" attribute, adjusted lag. Additionally, column names Y set corresponding time points.","code":""},{"path":"/reference/nmfkc.ar.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Construct observation and covariate matrices for a vector autoregressive model — nmfkc.ar","text":"","code":"nmfkc.ar(Y, degree = 1, intercept = TRUE)"},{"path":"/reference/nmfkc.ar.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Construct observation and covariate matrices for a vector autoregressive model — nmfkc.ar","text":"Y observation matrix (P x N) ts object. Y ts object (typically N x P), automatically transposed match (P x N) format. degree lag order autoregressive model. default 1. intercept Logical. TRUE (default), intercept term added covariate matrix.","code":""},{"path":"/reference/nmfkc.ar.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Construct observation and covariate matrices for a vector autoregressive model — nmfkc.ar","text":"list containing: Y Observation matrix (P x N_A) used NMF. Includes adjusted \"tsp_info\" attribute time-based column names. Covariate matrix (R x N_A) constructed according specified lag order. Includes adjusted \"tsp_info\" attribute time-based column names. .columns Index matrix used generate . degree.max Maximum lag order.","code":""},{"path":[]},{"path":"/reference/nmfkc.ar.predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Forecast future values for NMF-VAR model — nmfkc.ar.predict","title":"Forecast future values for NMF-VAR model — nmfkc.ar.predict","text":"nmfkc.ar.predict computes multi-step-ahead forecasts fitted NMF-VAR model using recursive forecasting. fitted model contains time series property information (nmfkc.ar), forecasted values appropriate time-based column names.","code":""},{"path":"/reference/nmfkc.ar.predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forecast future values for NMF-VAR model — nmfkc.ar.predict","text":"","code":"nmfkc.ar.predict(x, Y, degree = NULL, n.ahead = 1)"},{"path":"/reference/nmfkc.ar.predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forecast future values for NMF-VAR model — nmfkc.ar.predict","text":"x object class nmfkc (fitted model). Y historical observation matrix used fitting (least last degree columns). degree Optional integer. Lag order (D). NULL (default), inferred x$.attr (available) dimensions x$C. n.ahead Integer (>=1). Number steps ahead forecast.","code":""},{"path":"/reference/nmfkc.ar.predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forecast future values for NMF-VAR model — nmfkc.ar.predict","text":"list components: pred \\(P \\times n.ahead\\) matrix predicted values. Column names future time points time information available. time numeric vector future time points corresponding columns pred.","code":""},{"path":[]},{"path":"/reference/nmfkc.ar.stationarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Check stationarity of an NMF-VAR model — nmfkc.ar.stationarity","title":"Check stationarity of an NMF-VAR model — nmfkc.ar.stationarity","text":"nmfkc.ar.stationarity assesses dynamic stability VAR model computing spectral radius companion matrix. returns spectral radius logical indicator stationarity.","code":""},{"path":"/reference/nmfkc.ar.stationarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check stationarity of an NMF-VAR model — nmfkc.ar.stationarity","text":"","code":"nmfkc.ar.stationarity(x)"},{"path":"/reference/nmfkc.ar.stationarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check stationarity of an NMF-VAR model — nmfkc.ar.stationarity","text":"x return value nmfkc VAR model.","code":""},{"path":"/reference/nmfkc.ar.stationarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check stationarity of an NMF-VAR model — nmfkc.ar.stationarity","text":"list components: spectral.radius Numeric. spectral radius companion matrix. value less 1 indicates stationarity. stationary Logical. TRUE spectral radius less 1 (.e., system stationary), FALSE otherwise.","code":""},{"path":[]},{"path":"/reference/nmfkc.class.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a class (one-hot) matrix from a categorical vector — nmfkc.class","title":"Create a class (one-hot) matrix from a categorical vector — nmfkc.class","text":"nmfkc.class converts categorical factor vector class matrix (one-hot encoded representation), row corresponds category column corresponds observation.","code":""},{"path":"/reference/nmfkc.class.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a class (one-hot) matrix from a categorical vector — nmfkc.class","text":"","code":"nmfkc.class(x)"},{"path":"/reference/nmfkc.class.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a class (one-hot) matrix from a categorical vector — nmfkc.class","text":"x categorical vector factor.","code":""},{"path":"/reference/nmfkc.class.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a class (one-hot) matrix from a categorical vector — nmfkc.class","text":"binary matrix one row per unique category one column per observation. column exactly one entry equal 1, indicating category observation.","code":""},{"path":[]},{"path":"/reference/nmfkc.class.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a class (one-hot) matrix from a categorical vector — nmfkc.class","text":"","code":"# install.packages(\"remotes\") # remotes::install_github(\"ksatohds/nmfkc\") # Example. Y <- nmfkc.class(iris$Species) Y[,1:6] #>            1 2 3 4 5 6 #> setosa     1 1 1 1 1 1 #> versicolor 0 0 0 0 0 0 #> virginica  0 0 0 0 0 0"},{"path":"/reference/nmfkc.cv.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform k-fold cross-validation for NMF with kernel covariates — nmfkc.cv","title":"Perform k-fold cross-validation for NMF with kernel covariates — nmfkc.cv","text":"nmfkc.cv performs k-fold cross-validation tri-factorization model \\(Y \\approx X C = X B\\), \\(Y(P,N)\\) observation matrix, \\((R,N)\\) covariate (kernel) matrix, \\(X(P,Q)\\) basis matrix (\\(Q \\le \\min(P,N)\\)), \\(C(Q,R)\\) parameter matrix, \\(B(Q,N)\\) coefficient matrix (\\(B = C \\)). Given \\(Y\\) (optionally \\(\\)), \\(X\\) \\(C\\) fitted training split predictive performance evaluated held-split.","code":""},{"path":"/reference/nmfkc.cv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform k-fold cross-validation for NMF with kernel covariates — nmfkc.cv","text":"","code":"nmfkc.cv(Y, A = NULL, Q = 2, ...)"},{"path":"/reference/nmfkc.cv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform k-fold cross-validation for NMF with kernel covariates — nmfkc.cv","text":"Y Observation matrix. Covariate matrix. NULL, identity matrix used. Q Rank basis matrix \\(X\\); must satisfy \\(Q \\le \\min(P,N)\\). ... Additional arguments controlling CV internal nmfkc call: Y.weights Optional numeric matrix vector; 0 indicates missing/ignored values. div Number folds (\\(k\\)); default: 5. seed Integer seed reproducible partitioning; default: 123. shuffle Logical. TRUE (default), randomly shuffles samples (standard CV); FALSE, splits sequentially (block CV; recommended time series). Arguments passed nmfkc e.g., gamma (B.L1), epsilon, maxit, method (\"EU\" \"KL\"), X.restriction, X.init, etc.","code":""},{"path":"/reference/nmfkc.cv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform k-fold cross-validation for NMF with kernel covariates — nmfkc.cv","text":"list components: objfunc Mean loss per valid entry folds (MSE method=\"EU\"). sigma Residual standard error (RMSE). Available method=\"EU\"; scale Y. objfunc.block Loss fold. block Vector fold indices (1, …, div) assigned column \\(Y\\).","code":""},{"path":[]},{"path":"/reference/nmfkc.cv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform k-fold cross-validation for NMF with kernel covariates — nmfkc.cv","text":"","code":"# Example 1 (with explicit covariates): Y <- matrix(cars$dist, nrow = 1) A <- rbind(1, cars$speed) res <- nmfkc.cv(Y, A, Q = 1) res$objfunc #> [1] 286.3378  # Example 2 (kernel A and beta sweep): Y <- matrix(cars$dist, nrow = 1) U <- matrix(c(5, 10, 15, 20, 25), nrow = 1) V <- matrix(cars$speed, nrow = 1) betas <- 25:35/1000 obj <- numeric(length(betas)) for (i in seq_along(betas)) {   A <- nmfkc.kernel(U, V, beta = betas[i])   obj[i] <- nmfkc.cv(Y, A, Q = 1, div = 10)$objfunc } betas[which.min(obj)] #> [1] 0.031"},{"path":"/reference/nmfkc.denormalize.html","id":null,"dir":"Reference","previous_headings":"","what":"Denormalize a matrix from \\([0,1]\\) back to its original scale — nmfkc.denormalize","title":"Denormalize a matrix from \\([0,1]\\) back to its original scale — nmfkc.denormalize","text":"nmfkc.denormalize rescales matrix values \\([0,1]\\) back original scale using column-wise minima maxima reference matrix.","code":""},{"path":"/reference/nmfkc.denormalize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Denormalize a matrix from \\([0,1]\\) back to its original scale — nmfkc.denormalize","text":"","code":"nmfkc.denormalize(x, ref = x)"},{"path":"/reference/nmfkc.denormalize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Denormalize a matrix from \\([0,1]\\) back to its original scale — nmfkc.denormalize","text":"x numeric matrix (vector) values \\([0,1]\\) denormalized. ref reference matrix used obtain original column-wise minima maxima. Must number columns x.","code":""},{"path":"/reference/nmfkc.denormalize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Denormalize a matrix from \\([0,1]\\) back to its original scale — nmfkc.denormalize","text":"numeric matrix values transformed back original scale.","code":""},{"path":[]},{"path":"/reference/nmfkc.denormalize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Denormalize a matrix from \\([0,1]\\) back to its original scale — nmfkc.denormalize","text":"","code":"x <- nmfkc.normalize(iris[, -5]) x_recovered <- nmfkc.denormalize(x, iris[, -5]) apply(x_recovered - iris[, -5], 2, max) #> Sepal.Length  Sepal.Width Petal.Length  Petal.Width  #> 0.000000e+00 4.440892e-16 0.000000e+00 2.220446e-16"},{"path":"/reference/nmfkc.DOT.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Graphviz DOT Scripts for NMF or NMF-with-Covariates Models — nmfkc.DOT","title":"Generate Graphviz DOT Scripts for NMF or NMF-with-Covariates Models — nmfkc.DOT","text":"Produces Graphviz DOT script visualizing structure NMF model (\\(Y \\approx X C \\)) simplified forms. Supported visualization types: \"YX\" — Standard NMF view: latent factors \\(X\\) map observations \\(Y\\). \"YA\" — Direct regression view: covariates \\(\\) map directly \\(Y\\) using combined coefficient matrix \\(X C\\). \"YXA\" — Full tri-factorization: \\(\\rightarrow C \\rightarrow X \\rightarrow Y\\). Edge widths scaled coefficient magnitude, nodes edges threshold omitted visualization.","code":""},{"path":"/reference/nmfkc.DOT.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Graphviz DOT Scripts for NMF or NMF-with-Covariates Models — nmfkc.DOT","text":"","code":"nmfkc.DOT(   x,   type = c(\"YX\", \"YA\", \"YXA\"),   threshold = 0.01,   rankdir = \"LR\",   fill = TRUE,   weight_scale = 5,   weight_scale_ax = weight_scale,   weight_scale_xy = weight_scale,   weight_scale_ay = weight_scale,   Y.label = NULL,   X.label = NULL,   A.label = NULL,   Y.title = \"Observation (Y)\",   X.title = \"Basis (X)\",   A.title = \"Covariates (A)\" )"},{"path":"/reference/nmfkc.DOT.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Graphviz DOT Scripts for NMF or NMF-with-Covariates Models — nmfkc.DOT","text":"x return value nmfkc, containing matrices X, B, optionally C. type Character string specifying visualization style: one \"YX\", \"YA\", \"YXA\". threshold Minimum coefficient magnitude display edge. rankdir Graphviz rank direction (e.g., \"LR\", \"TB\"). fill Logical; whether nodes drawn filled shapes. weight_scale Base scaling factor edge widths. weight_scale_ax Scaling factor edges \\(\\rightarrow X\\) (type \"YXA\"). weight_scale_xy Scaling factor edges \\(X \\rightarrow Y\\). weight_scale_ay Scaling factor edges \\(\\rightarrow Y\\) (type \"YA\"). Y.label Optional character vector labels Y nodes. X.label Optional character vector labels X (latent factor) nodes. .label Optional character vector labels (covariate) nodes. Y.title Cluster title Y nodes. X.title Cluster title X nodes. .title Cluster title nodes.","code":""},{"path":"/reference/nmfkc.DOT.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Graphviz DOT Scripts for NMF or NMF-with-Covariates Models — nmfkc.DOT","text":"character string representing Graphviz DOT script.","code":""},{"path":[]},{"path":"/reference/nmfkc.ecv.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform Element-wise Cross-Validation (Wold's CV) — nmfkc.ecv","title":"Perform Element-wise Cross-Validation (Wold's CV) — nmfkc.ecv","text":"nmfkc.ecv performs k-fold cross-validation randomly holding individual elements data matrix (element-wise), assigning weight 0 via Y.weights, evaluating reconstruction error held-elements. method (also known Wold's CV) theoretically robust determining optimal rank (Q) NMF. function supports vector input Q, allowing simultaneous evaluation multiple ranks folds.","code":""},{"path":"/reference/nmfkc.ecv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform Element-wise Cross-Validation (Wold's CV) — nmfkc.ecv","text":"","code":"nmfkc.ecv(Y, A = NULL, Q = 1:3, div = 5, seed = 123, ...)"},{"path":"/reference/nmfkc.ecv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform Element-wise Cross-Validation (Wold's CV) — nmfkc.ecv","text":"Y Observation matrix. Covariate matrix. Q Vector ranks evaluate (e.g., 1:5). div Number folds (default: 5). seed Integer seed reproducibility. ... Additional arguments passed nmfkc (e.g., method=\"EU\").","code":""},{"path":"/reference/nmfkc.ecv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform Element-wise Cross-Validation (Wold's CV) — nmfkc.ecv","text":"list components: objfunc Numeric vector containing Mean Squared Error (MSE) Q. sigma Numeric vector containing Residual Standard Error (RMSE) Q. available method=\"EU\". objfunc.fold List length equal Q vector. element contains MSE values k folds. folds list length div, containing linear indices held-elements fold (shared across Q).","code":""},{"path":[]},{"path":"/reference/nmfkc.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimize NMF with kernel covariates (Full Support for Missing Values) — nmfkc","title":"Optimize NMF with kernel covariates (Full Support for Missing Values) — nmfkc","text":"nmfkc fits nonnegative matrix factorization kernel covariates tri-factorization model \\(Y \\approx X C = X B\\). function supports two major input modes: Matrix Mode (Existing): nmfkc(Y=matrix, =matrix, ...) Formula Mode (New): nmfkc(formula=Y_vars ~ A_vars, data=df, rank=Q, ...) rank basis matrix can specified using either rank argument (preferred formula mode) hidden Q argument (backward compatibility).","code":""},{"path":"/reference/nmfkc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimize NMF with kernel covariates (Full Support for Missing Values) — nmfkc","text":"","code":"nmfkc(Y, A = NULL, rank = NULL, data, epsilon = 1e-04, maxit = 5000, ...)"},{"path":"/reference/nmfkc.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Optimize NMF with kernel covariates (Full Support for Missing Values) — nmfkc","text":"Satoh, K. (2024). Applying Non-negative Matrix Factorization Covariates Longitudinal Data Growth Curve Model. arXiv:2403.05359. https://arxiv.org/abs/2403.05359","code":""},{"path":"/reference/nmfkc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimize NMF with kernel covariates (Full Support for Missing Values) — nmfkc","text":"Y Observation matrix, formula object data supplied. Covariate matrix. Default NULL (covariates). rank Integer. rank basis matrix \\(X\\) (Q). Preferred Q. data Optional. data frame variables formula taken. epsilon Positive convergence tolerance. maxit Maximum number iterations. ... Additional arguments passed fine-tuning regularization, initialization, constraints, output control. includes backward-compatible arguments Q method. Y.weights: Optional numeric matrix (P x N) vector (length N). 0 indicates missing/ignored values. NULL (default), weights automatically set 0 NAs Y, 1 otherwise. X.L2.ortho: Nonnegative penalty parameter orthogonality \\(X\\) (default: 0). minimizes -diagonal elements Gram matrix \\(X^\\top X\\), reducing correlation basis vectors (conceptually minimizing \\(\\| X^\\top X - \\mathrm{diag}(X^\\top X) \\|_F^2\\)). (Formerly lambda.ortho). B.L1: Nonnegative penalty parameter L1 regularization \\(B = C \\) (default: 0). Promotes sparsity coefficients. (Formerly gamma). C.L1: Nonnegative penalty parameter L1 regularization \\(C\\) (default: 0). Promotes sparsity parameter matrix. (Formerly lambda). Q: Backward-compatible name rank basis matrix (Q). method: Objective function: Euclidean distance \"EU\" (default) Kullback–Leibler divergence \"KL\". X.restriction: Constraint columns \\(X\\). Options: \"colSums\" (default), \"colSqSums\", \"totalSum\", \"fixed\". X.init: Method initializing basis matrix \\(X\\). Options: \"kmeans\" (default), \"runif\", \"nndsvd\", user-specified matrix. nstart: Number random starts kmeans initializing \\(X\\) (default: 1). seed: Integer seed reproducibility (default: 123). prefix: Prefix column names \\(X\\) row names \\(B\\) (default: \"Basis\"). print.trace: Logical. TRUE, prints progress every 10 iterations (default: FALSE). print.dims: Logical. TRUE (default), prints matrix dimensions elapsed time. save.time: Logical. TRUE (default), skips post-computations (e.g., CPCC, silhouette) save time. save.memory: Logical. TRUE, performs essential computations (implies save.time = TRUE) reduce memory usage (default: FALSE).","code":""},{"path":"/reference/nmfkc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimize NMF with kernel covariates (Full Support for Missing Values) — nmfkc","text":"list components: call matched call, captured match.call(). dims character string summarizing matrix dimensions model. runtime character string summarizing computation time. X Basis matrix. Column normalization depends X.restriction. B Coefficient matrix \\(B = C \\). XB Fitted values \\(Y\\). C Parameter matrix. B.prob Soft-clustering probabilities derived columns \\(B\\). B.cluster Hard-clustering labels (argmax \\(B.prob\\) column). X.prob Row-wise soft-clustering probabilities derived \\(X\\). X.cluster Hard-clustering labels (argmax \\(X.prob\\) row). .attr List attributes input covariate matrix , containing metadata like lag order intercept status created nmfkc.ar nmfkc.kernel. objfunc Final objective value. objfunc.iter Objective values iteration. r.squared Coefficient determination \\(R^2\\) \\(Y\\) \\(X B\\). sigma residual standard error, representing typical deviation observed values \\(Y\\) fitted values \\(X B\\). criterion list selection criteria, including ICp, CPCC, silhouette, AIC, BIC.","code":""},{"path":"/reference/nmfkc.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimize NMF with kernel covariates (Full Support for Missing Values) — nmfkc","text":"Ding, C., Li, T., Peng, W., & Park, H. (2006). Orthogonal Nonnegative Matrix Tri-Factorizations Clustering. Proceedings 12th ACM SIGKDD International Conference Knowledge Discovery Data Mining (pp. 126–135). doi:10.1145/1150402.1150420  Potthoff, R. F., & Roy, S. N. (1964). generalized multivariate analysis variance model useful especially growth curve problems. Biometrika, 51, 313–326. doi:10.2307/2334137","code":""},{"path":[]},{"path":"/reference/nmfkc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimize NMF with kernel covariates (Full Support for Missing Values) — nmfkc","text":"","code":"# install.packages(\"remotes\") # remotes::install_github(\"ksatohds/nmfkc\") # Example 1. Matrix Mode (Existing) library(nmfkc) X <- cbind(c(1,0,1),c(0,1,0)) B <- cbind(c(1,0),c(0,1),c(1,1)) Y <- X %*% B rownames(Y) <- paste0(\"P\",1:nrow(Y)) colnames(Y) <- paste0(\"N\",1:ncol(Y)) print(X); print(B); print(Y) #>      [,1] [,2] #> [1,]    1    0 #> [2,]    0    1 #> [3,]    1    0 #>      [,1] [,2] [,3] #> [1,]    1    0    1 #> [2,]    0    1    1 #>    N1 N2 N3 #> P1  1  0  1 #> P2  0  1  1 #> P3  1  0  1 library(nmfkc) res <- nmfkc(Y,Q=2,epsilon=1e-6) #> Y(3,3)~X(3,2)B(2,3)... #> 0sec res$X #>    Basis1      Basis2 #> P1      0 0.498047869 #> P2      1 0.003904261 #> P3      0 0.498047869 res$B #>              N1           N2        N3 #> Basis1 0.000000 0.9999995176 0.9920988 #> Basis2 2.007838 0.0001206861 2.0079012  # Example 2. Formula Mode (New) # dummy_data <- data.frame(Y1=rpois(10,5), Y2=rpois(10,10), A1=1:10, A2=rnorm(10,5)) # res_f <- nmfkc(Y1 + Y2 ~ A1 + A2, data=dummy_data, rank=2)"},{"path":"/reference/nmfkc.kernel.beta.cv.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimize beta of the Gaussian kernel function by cross-validation — nmfkc.kernel.beta.cv","title":"Optimize beta of the Gaussian kernel function by cross-validation — nmfkc.kernel.beta.cv","text":"nmfkc.kernel.beta.cv selects optimal beta parameter kernel function applying cross-validation set candidate values.","code":""},{"path":"/reference/nmfkc.kernel.beta.cv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimize beta of the Gaussian kernel function by cross-validation — nmfkc.kernel.beta.cv","text":"","code":"nmfkc.kernel.beta.cv(Y, Q = 2, U, V = NULL, beta = NULL, plot = TRUE, ...)"},{"path":"/reference/nmfkc.kernel.beta.cv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimize beta of the Gaussian kernel function by cross-validation — nmfkc.kernel.beta.cv","text":"Y Observation matrix \\(Y(P,N)\\). Q Rank basis matrix. Must satisfy \\(Q \\le \\min(P,N)\\). U Covariate matrix \\(U(K,N) = (u_1, \\dots, u_N)\\). row may normalized advance. V Covariate matrix \\(V(K,M) = (v_1, \\dots, v_M)\\), typically used prediction. NULL, default U. beta numeric vector candidate kernel parameters evaluate via cross-validation. plot Logical. TRUE (default), plots objective function values candidate beta. ... Additional arguments passed nmfkc.cv.","code":""},{"path":"/reference/nmfkc.kernel.beta.cv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimize beta of the Gaussian kernel function by cross-validation — nmfkc.kernel.beta.cv","text":"list components: beta beta value minimizes cross-validation objective function. objfunc Objective function values candidate beta.","code":""},{"path":"/reference/nmfkc.kernel.beta.cv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimize beta of the Gaussian kernel function by cross-validation — nmfkc.kernel.beta.cv","text":"","code":"# install.packages(\"remotes\") # remotes::install_github(\"ksatohds/nmfkc\") # Example. Y <- matrix(cars$dist,nrow=1) U <- matrix(c(5,10,15,20,25),nrow=1) V <- matrix(cars$speed,nrow=1) nmfkc.kernel.beta.cv(Y,Q=1,U,V,beta=25:30/1000) #> beta=0.025... #> 0sec #> beta=0.026... #> 0sec #> beta=0.027... #> 0sec #> beta=0.028... #> 0sec #> beta=0.029... #> 0sec #> beta=0.03... #> 0sec  #> $beta #> [1] 0.028 #>  #> $objfunc #>    0.025    0.026    0.027    0.028    0.029     0.03  #> 260.5817 259.9821 259.5190 259.4072 259.4619 259.6314  #>  A <- nmfkc.kernel(U,V,beta=28/1000) result <- nmfkc(Y,A,Q=1) #> Y(1,50)~X(1,1)C(1,5)A(5,50)=XB(1,50)... #> 0sec plot(as.vector(V),as.vector(Y)) lines(as.vector(V),as.vector(result$XB),col=2,lwd=2)"},{"path":"/reference/nmfkc.kernel.beta.nearest.med.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate kernel parameter beta from covariates — nmfkc.kernel.beta.nearest.med","title":"Estimate kernel parameter beta from covariates — nmfkc.kernel.beta.nearest.med","text":"nmfkc.kernel.beta.nearest.med estimates Gaussian kernel parameter \\(\\beta\\) computing median nearest-neighbor distances among covariates. useful setting scale parameter kernel-based NMF covariates.","code":""},{"path":"/reference/nmfkc.kernel.beta.nearest.med.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate kernel parameter beta from covariates — nmfkc.kernel.beta.nearest.med","text":"","code":"nmfkc.kernel.beta.nearest.med(U, block_size = 1000)"},{"path":"/reference/nmfkc.kernel.beta.nearest.med.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate kernel parameter beta from covariates — nmfkc.kernel.beta.nearest.med","text":"U covariate matrix \\(U(K,N)=(u_1,\\dots,u_N)\\), column corresponds individual. row may normalized advance. block_size number samples process . \\(N \\le 1000\\), automatically set \\(N\\).","code":""},{"path":"/reference/nmfkc.kernel.beta.nearest.med.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate kernel parameter beta from covariates — nmfkc.kernel.beta.nearest.med","text":"list components: beta estimated kernel parameter \\(\\beta=1/(2 d_{med}^2)\\) beta_candidates numeric vector candidate values obtained multiplying estimate \\(\\beta\\) powers 10, .e.\\ \\(\\{\\beta \\cdot 10^{-2},\\,\\beta \\cdot 10^{-1},\\,\\beta,\\,\\beta \\cdot 10^{1}\\}\\) dist_median median nearest-neighbor distance block_size_used actual block size used computation","code":""},{"path":"/reference/nmfkc.kernel.beta.nearest.med.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Estimate kernel parameter beta from covariates — nmfkc.kernel.beta.nearest.med","text":"function computes pairwise squared distances columns \\(U\\), excludes self-distances, takes median nearest-neighbor distances (square root). median used set \\(\\beta\\).","code":""},{"path":[]},{"path":"/reference/nmfkc.kernel.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a kernel matrix from covariates — nmfkc.kernel","title":"Create a kernel matrix from covariates — nmfkc.kernel","text":"nmfkc.kernel constructs kernel matrix covariate matrices. supports Gaussian, Exponential, Periodic, Linear, Normalized Linear, Polynomial kernels.","code":""},{"path":"/reference/nmfkc.kernel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a kernel matrix from covariates — nmfkc.kernel","text":"","code":"nmfkc.kernel(   U,   V = NULL,   kernel = c(\"Gaussian\", \"Exponential\", \"Periodic\", \"Linear\", \"NormalizedLinear\",     \"Polynomial\"),   ... )"},{"path":"/reference/nmfkc.kernel.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Create a kernel matrix from covariates — nmfkc.kernel","text":"Satoh, K. (2024). Applying Non-negative Matrix Factorization Covariates Longitudinal Data Growth Curve Model. arXiv preprint arXiv:2403.05359. https://arxiv.org/abs/2403.05359","code":""},{"path":"/reference/nmfkc.kernel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a kernel matrix from covariates — nmfkc.kernel","text":"U Covariate matrix \\(U(K,N) = (u_1, \\dots, u_N)\\). row may normalized advance. V Covariate matrix \\(V(K,M) = (v_1, \\dots, v_M)\\), typically used prediction. NULL, default U. kernel Kernel function use. Default \"Gaussian\". Options \"Gaussian\", \"Exponential\", \"Periodic\", \"Linear\", \"NormalizedLinear\", \"Polynomial\". ... Additional arguments passed specific kernel function (e.g., beta, degree).","code":""},{"path":"/reference/nmfkc.kernel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a kernel matrix from covariates — nmfkc.kernel","text":"Kernel matrix \\((N,M)\\).","code":""},{"path":[]},{"path":"/reference/nmfkc.kernel.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a kernel matrix from covariates — nmfkc.kernel","text":"","code":"# install.packages(\"remotes\") # remotes::install_github(\"ksatohds/nmfkc\") # Example. Y <- matrix(cars$dist,nrow=1) U <- matrix(c(5,10,15,20,25),nrow=1) V <- matrix(cars$speed,nrow=1) A <- nmfkc.kernel(U,V,beta=28/1000) dim(A) #> [1]  5 50 result <- nmfkc(Y,A,Q=1) #> Y(1,50)~X(1,1)C(1,5)A(5,50)=XB(1,50)... #> 0sec plot(as.vector(V),as.vector(Y)) lines(as.vector(V),as.vector(result$XB),col=2,lwd=2)"},{"path":"/reference/nmfkc.normalize.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalize a matrix to the range \\([0,1]\\) — nmfkc.normalize","title":"Normalize a matrix to the range \\([0,1]\\) — nmfkc.normalize","text":"nmfkc.normalize rescales values matrix lie 0 1 using column-wise minimum maximum values reference matrix.","code":""},{"path":"/reference/nmfkc.normalize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalize a matrix to the range \\([0,1]\\) — nmfkc.normalize","text":"","code":"nmfkc.normalize(x, ref = x)"},{"path":"/reference/nmfkc.normalize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Normalize a matrix to the range \\([0,1]\\) — nmfkc.normalize","text":"x numeric matrix (vector) normalized. ref reference matrix column-wise minima maxima taken. Default x.","code":""},{"path":"/reference/nmfkc.normalize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Normalize a matrix to the range \\([0,1]\\) — nmfkc.normalize","text":"matrix dimensions x, column rescaled \\([0,1]\\) range.","code":""},{"path":[]},{"path":"/reference/nmfkc.normalize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Normalize a matrix to the range \\([0,1]\\) — nmfkc.normalize","text":"","code":"# install.packages(\"remotes\") # remotes::install_github(\"ksatohds/nmfkc\") # Example. x <- nmfkc.normalize(iris[,-5]) apply(x,2,range) #>      Sepal.Length Sepal.Width Petal.Length Petal.Width #> [1,]            0           0            0           0 #> [2,]            1           1            1           1"},{"path":"/reference/nmfkc.rank.html","id":null,"dir":"Reference","previous_headings":"","what":"Rank selection diagnostics with graphical output — nmfkc.rank","title":"Rank selection diagnostics with graphical output — nmfkc.rank","text":"nmfkc.rank provides diagnostic criteria selecting rank (\\(Q\\)) NMF kernel covariates. Several model selection measures computed (e.g., R-squared, silhouette, CPCC, ARI), results can visualized plot. default (save.time = FALSE), function also computes Element-wise Cross-Validation error (Wold's CV Sigma) using nmfkc.ecv. plot explicitly marks \"BEST\" rank based two criteria: Elbow Method (Red): Based curvature R-squared values (always computed Q > 2). Min RMSE (Blue): Based minimum Element-wise CV Sigma (save.time=FALSE).","code":""},{"path":"/reference/nmfkc.rank.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rank selection diagnostics with graphical output — nmfkc.rank","text":"","code":"nmfkc.rank(Y, A = NULL, rank = 1:2, save.time = FALSE, plot = TRUE, ...)"},{"path":"/reference/nmfkc.rank.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rank selection diagnostics with graphical output — nmfkc.rank","text":"Y Observation matrix. Covariate matrix. NULL, identity matrix used. rank vector candidate ranks evaluated. save.time Logical. TRUE, skips heavy computations like Element-wise CV. Default FALSE (computes everything). plot Logical. TRUE (default), draws plot diagnostic criteria. ... Additional arguments passed nmfkc nmfkc.ecv. Q: (Deprecated) Alias rank.","code":""},{"path":"/reference/nmfkc.rank.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rank selection diagnostics with graphical output — nmfkc.rank","text":"list containing: rank.best estimated optimal rank. Prioritizes ECV minimum available, otherwise R-squared Elbow. criteria data frame containing diagnostic metrics rank.","code":""},{"path":"/reference/nmfkc.rank.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Rank selection diagnostics with graphical output — nmfkc.rank","text":"Brunet, J.P., Tamayo, P., Golub, T.R., Mesirov, J.P. (2004). Metagenes molecular pattern discovery using matrix factorization. Proc. Natl. Acad. Sci. USA, 101, 4164–4169. doi:10.1073/pnas.0308531101  Punera, K., & Ghosh, J. (2008). Consensus-based ensembles soft clusterings. Applied Artificial Intelligence, 22(7–8), 780–810. doi:10.1080/08839510802170546","code":""},{"path":[]},{"path":"/reference/nmfkc.rank.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rank selection diagnostics with graphical output — nmfkc.rank","text":"","code":"# install.packages(\"remotes\") # remotes::install_github(\"ksatohds/nmfkc\") # Example. library(nmfkc) Y <- t(iris[,-5]) # Full run (default) nmfkc.rank(Y, rank=1:4) #> Y(4,150)~X(4,1)B(1,150)... #> 0sec #> Y(4,150)~X(4,2)B(2,150)... #> 0sec #> Y(4,150)~X(4,3)B(3,150)... #> 0sec #> Y(4,150)~X(4,4)B(4,150)... #> 0sec #> Running Element-wise CV (this may take time)... #> Performing Element-wise CV for Q = 1,2,3,4 (5-fold)... #> Y(4,150)~X(4,1)B(1,150)... #> 0sec #> Y(4,150)~X(4,1)B(1,150)... #> 0sec #> Y(4,150)~X(4,1)B(1,150)... #> 0sec #> Y(4,150)~X(4,1)B(1,150)... #> 0sec #> Y(4,150)~X(4,1)B(1,150)... #> 0sec #> Y(4,150)~X(4,2)B(2,150)... #> 0sec #> Y(4,150)~X(4,2)B(2,150)... #> 0sec #> Y(4,150)~X(4,2)B(2,150)... #> 0sec #> Y(4,150)~X(4,2)B(2,150)... #> 0sec #> Y(4,150)~X(4,2)B(2,150)... #> 0sec #> Y(4,150)~X(4,3)B(3,150)... #> 0sec #> Y(4,150)~X(4,3)B(3,150)... #> 0sec #> Y(4,150)~X(4,3)B(3,150)... #> 0sec #> Y(4,150)~X(4,3)B(3,150)... #> 0sec #> Y(4,150)~X(4,3)B(3,150)... #> 0sec #> Y(4,150)~X(4,4)B(4,150)... #> 0sec #> Y(4,150)~X(4,4)B(4,150)... #> 0sec #> Y(4,150)~X(4,4)B(4,150)... #> 0sec #> Y(4,150)~X(4,4)B(4,150)... #> 0sec #> Y(4,150)~X(4,4)B(4,150)... #> 0sec  #> $rank.best #> [1] 4 #>  #> $criteria #>   rank r.squared      ICp         AIC        BIC B.prob.sd.min #> 1    1 0.8586795  52.4309   -50.91409   621.8162     0.0000000 #> 2    2 0.9933479 102.3992 -1579.36199  -233.9015     0.2443194 #> 3    3 0.9984511 153.9677 -2147.71982  -129.5291     0.1300233 #> 4    4 0.9999888 202.0625 -4800.29737 -2109.3764     0.1191841 #>   B.prob.entropy.mean B.prob.max.mean       ARI silhouette      CPCC  dist.cor #> 1           0.0000000       1.0000000        NA         NA        NA 0.9410181 #> 2           0.7980677       0.7075007        NA  0.8692814 0.9264254 0.9746472 #> 3           0.8336790       0.5548794 0.5623250  0.5358708 0.9193853 0.9489567 #> 4           0.8887089       0.4003933 0.5404919  0.3049893 0.8966046 0.9464434 #>   sigma.ecv #> 1 1.1694153 #> 2 0.7997277 #> 3 0.7786008 #> 4 0.7674663 #>  # Fast run (skip ECV) nmfkc.rank(Y, rank=1:4, save.time=TRUE) #> Y(4,150)~X(4,1)B(1,150)... #> 0sec #> Y(4,150)~X(4,2)B(2,150)... #> 0sec #> Y(4,150)~X(4,3)B(3,150)... #> 0sec #> Y(4,150)~X(4,4)B(4,150)... #> 0sec  #> $rank.best #> [1] 2 #>  #> $criteria #>   rank r.squared      ICp         AIC        BIC B.prob.sd.min #> 1    1 0.8586795  52.4309   -50.91409   621.8162     0.0000000 #> 2    2 0.9933479 102.3992 -1579.36199  -233.9015     0.2443194 #> 3    3 0.9984511 153.9677 -2147.71982  -129.5291     0.1300233 #> 4    4 0.9999888 202.0625 -4800.29737 -2109.3764     0.1191841 #>   B.prob.entropy.mean B.prob.max.mean       ARI silhouette CPCC dist.cor #> 1           0.0000000       1.0000000        NA         NA   NA       NA #> 2           0.7980677       0.7075007        NA         NA   NA       NA #> 3           0.8336790       0.5548794 0.5623250         NA   NA       NA #> 4           0.8887089       0.4003933 0.5404919         NA   NA       NA #>   sigma.ecv #> 1        NA #> 2        NA #> 3        NA #> 4        NA #>"},{"path":"/reference/nmfkc.residual.plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Diagnostics: Original, Fitted, and Residual Matrices as Heatmaps — nmfkc.residual.plot","title":"Plot Diagnostics: Original, Fitted, and Residual Matrices as Heatmaps — nmfkc.residual.plot","text":"function generates side--side plot three heatmaps: original observation matrix Y, fitted matrix XB (NMF), residual matrix E (Y - XB). visualization aids diagnosing whether chosen rank Q adequate assessing residual matrix E appears random noise. axis labels (X-axis: Samples, Y-axis: Features) integrated main title plot maximize plot area, reflecting compact layout settings.","code":""},{"path":"/reference/nmfkc.residual.plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Diagnostics: Original, Fitted, and Residual Matrices as Heatmaps — nmfkc.residual.plot","text":"","code":"nmfkc.residual.plot(   Y,   result,   Y_XB_palette = (grDevices::colorRampPalette(c(\"white\", \"orange\", \"red\")))(256),   E_palette = (grDevices::colorRampPalette(c(\"blue\", \"white\", \"red\")))(256),   ... )"},{"path":"/reference/nmfkc.residual.plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Diagnostics: Original, Fitted, and Residual Matrices as Heatmaps — nmfkc.residual.plot","text":"Y original observation matrix (P x N). result result object returned nmfkc function. Y_XB_palette vector colors used Y XB heatmaps. Defaults white-orange-red gradient. E_palette vector colors used residuals (E) heatmap. Defaults blue-white-red gradient. ... Additional graphical parameters passed internal image calls.","code":""},{"path":"/reference/nmfkc.residual.plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Diagnostics: Original, Fitted, and Residual Matrices as Heatmaps — nmfkc.residual.plot","text":"NULL. function generates plot.","code":""},{"path":"/reference/plot.nmfkc.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot method for objects of class nmfkc — plot.nmfkc","title":"Plot method for objects of class nmfkc — plot.nmfkc","text":"plot.nmfkc produces diagnostic plot return value nmfkc, showing objective function across iterations.","code":""},{"path":"/reference/plot.nmfkc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot method for objects of class nmfkc — plot.nmfkc","text":"","code":"# S3 method for class 'nmfkc' plot(x, ...)"},{"path":"/reference/plot.nmfkc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot method for objects of class nmfkc — plot.nmfkc","text":"x object class nmfkc, .e., return value nmfkc. ... Additional arguments passed base plot function.","code":""},{"path":"/reference/predict.nmfkc.html","id":null,"dir":"Reference","previous_headings":"","what":"Prediction method for objects of class nmfkc — predict.nmfkc","title":"Prediction method for objects of class nmfkc — predict.nmfkc","text":"predict.nmfkc generates predictions object class nmfkc, either using fitted covariates new covariate matrix.","code":""},{"path":"/reference/predict.nmfkc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prediction method for objects of class nmfkc — predict.nmfkc","text":"","code":"# S3 method for class 'nmfkc' predict(object, newA = NULL, type = \"response\", ...)"},{"path":"/reference/predict.nmfkc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prediction method for objects of class nmfkc — predict.nmfkc","text":"object object class nmfkc, .e., return value nmfkc. newA Optional. new covariate matrix used prediction. type Type prediction return. Options \"response\", \"prob\", \"class\". ... arguments passed methods.","code":""},{"path":[]},{"path":"/news/index.html","id":"graphviz-dot-output-consolidation-and-cleanup-0-5-8","dir":"Changelog","previous_headings":"","what":"Graphviz DOT Output Consolidation and Cleanup","title":"nmfkc 0.5.8","text":"Harmonized DOT-generating functions (nmf.sem.DOT, nmfkc.DOT, nmfkc.ar.DOT) consistent structure, naming conventions, visualization logic. Standardized node edge formatting rules, including unified cluster behavior, color schemes, edge-scaling conventions. Implemented threshold-aware coefficient labeling displayed numerical precision aligns visualization threshold, preventing misleadingly detailed labels. Removed unused redundant DOT fragments improved compatibility across Graphviz engines. Enhanced layout readability consistent indentation, node grouping, suppression isolated nodes specific visualization modes (e.g., type = \"YA\" nmfkc.DOT). Refactored expanded internal DOT helper functions (.nmfkc_dot_format_coef, .nmfkc_dot_digits_from_threshold, .nmfkc_dot_cluster_nodes, etc.) better maintainability uniform behavior. New Function: Implemented nmfkc.ecv() Element-wise Cross-Validation (Wold’s CV). function randomly masks elements observation matrix evaluate structural reconstruction error. provides statistically robust criterion rank selection, avoiding monotonic error decrease often seen standard column-wise CV. Supports vector input rank evaluate multiple ranks simultaneously. Missing Value & Weight Support: nmfkc() nmfkc.cv() now fully support missing values (NA) observation weights via hidden argument Y.weights (passed ...). Y contains NAs, automatically detected masked (assigned weight 0) optimization. Rank Selection Diagnostics (nmfkc.rank): Dual-Axis Visualization: plot now displays fitting metrics (R2R^2, etc.) left axis ECV Sigma (RMSE) right axis (blue line). Elbow: Geometric elbow point R2R^2 curve. Min: Minimum error point Element-wise CV. save.time defaults FALSE, enabling robust Element-wise CV calculation default. Argument Standardization: Unified rank argument name rank across functions (nmfkc, nmfkc.cv, nmfkc.ecv, nmfkc.rank). legacy argument Q still supported backward compatibility internally mapped rank. Summary Improvements: Sparsity Basis (XX) Coefficients (BB). Clustering Entropy (indicating “Crisp” vs “Ambiguous” clustering). Clustering Crispness (Mean Max Probability). Number percentage missing values YY. Improvements: Added validation check nmfkc.ar() ensure input Y missing values (propagated covariate matrix VAR models). Refined nmfkc.residual.plot() layout margins better visibility titles. Updated documentation reflect changes. Regularization Update: regularization scheme revised L2 (ridge) L1 (lasso-type) penalties. gamma now controls L1 penalty coefficient matrix ( B = C ), promoting sparsity sample-wise coefficients. new argument lambda added control L1 penalty parameter matrix ( C ), encouraging sparsity shared template structure. parameters can passed ellipsis (...) nmfkc() related functions. Function Signature Simplification:** Many less-frequently used arguments nmfkc() (e.g., gamma, X.restriction, X.init) nmfkc.cv() (e.g., div, seed) moved ellipsis (...) cleaner function signature. Performance Improvement: internal function .silhouette.simple vectorized optimized reduce computational cost, particularly calculation () b(). Removed fast.calc option nmfkc() function. Added X.init argument nmfkc() function, allowing selection 'kmeans' 'nndsvd' initialization methods. penalty term changed tr(CC') tr(BB') = tr(CAA'C'). Implemented internal .z xnorm functions. Added fast.calc option nmfkc() function. Optimized internal calculations improved performance. Updated citation(\"nmfkc\") added AIC/BIC output. Implemented nmfkc.ar.stationarity() function. Modified z() function. Used crossprod() faster matrix multiplication. Implemented nmfkc.ar.DOT() function. Added logic sort columns X form unit matrix special cases. Implemented nmfkc.kernel.beta.cv() nmfkc.ar.degree.cv() functions. Set default column names X Basis1, Basis2, etc. Added X.prob X.cluster return object. Skipped CPCC silhouette calculations save.time = TRUE. Added prototype nmfkc.ar() function. Added criterion argument nmfkc() function support multiple criteria. Updated nmfkc.rank() function. Added criterion argument nmfkc.rank() function. Implemented save.time argument. Implemented nmfkc.rank() function. Implemented nstart option kmeans() function. Added experimental implementation nmfkc.rank() function. Removed zero-variance columns rows warning. Added source references documentation. Renamed several components clarity: nmfkcreg nmfkc create.kernel nmfkc.kernel nmfkcreg.cv nmfkc.cv P B.prob cluster B.cluster unit X.column trace print.trace dims print.dims Added r.squared argument nmfkcreg.cv() function. nmfkcreg(): Added dims argument check matrix sizes. Added unit argument normalize basis matrix columns. Modified create.kernel() function support prediction. Updated examples GitHub. Removed YHAT return value; use XB instead. Added cluster return value hard clustering.","code":""}]
