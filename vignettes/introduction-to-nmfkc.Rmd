---
title: "Introduction to nmfkc"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to nmfkc}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 6,
  fig.height = 4
)
```

## Introduction

Welcome to the `nmfkc` package\! This vignette provides a basic introduction to the core function of the package, `nmfkc()`.

Non-negative Matrix Factorization (NMF) is a method to approximate an observation matrix $Y$ as a product of a basis matrix $X$ and a coefficient matrix $B$:
$$Y \approx X B$$
A key feature of NMF is that all matrices must have non-negative elements, making the result parts-based and easily interpretable.

The `nmfkc` package extends this framework by allowing for **covariates** ($Y \approx XCA$) and supporting **missing values**.

This vignette covers:

1.  **Basic NMF**: Decomposing a matrix without covariates.
2.  **Visualization**: Checking convergence and results.
3.  **Handling Missing Values**: Demonstrating robustness against `NA`s.

-----

## 1\. A Simple Example (Basic NMF)

First, let's load the `nmfkc` package.

```{r load-package}
library(nmfkc)
```

### Creating the Data

We create a simple observation matrix `Y` ($3 \times 3$) which is a product of a basis matrix `X` ($3 \times 2$) and a coefficient matrix `B` ($2 \times 3$).

```{r create-data}
# Define the original basis and coefficient matrices
(X_orig <- cbind(c(1, 0, 1), c(0, 1, 0)))
(B_orig <- cbind(c(1, 0), c(0, 1), c(1, 1)))

# Create the observation matrix Y
(Y <- X_orig %*% B_orig)
```

Our goal is to recover the structure of `X` and `B` from `Y`.

### Running NMF

We use the `nmfkc()` function. We specify the observation matrix `Y` and the desired **rank** (number of bases, formerly `Q`).

```{r run-nmfkc}
# Perform NMF with rank = 2
res <- nmfkc(Y, rank = 2, epsilon = 1e-6)
```

### Checking the Results

We can examine the estimated basis matrix `res$X` and coefficient matrix `res$B`.

```{r check-results}
# Estimated basis matrix
res$X

# Estimated coefficient matrix
res$B
```

The `nmfkc()` function successfully recovered the pattern (note that the order of columns in `X` may differ from the original, which is normal in NMF).

### Visualization

We can plot the result object to see the convergence of the objective function (residual sum of squares).

```{r plot-convergence}
# Plot optimization history
plot(res, main = "Convergence of NMF")
```

We can also visualize the fitted matrix using a heatmap.

```{r plot-heatmap, fig.width=7, fig.height=3}
# Compare Original Y and Fitted XB
par(mfrow = c(1, 2))
image(t(Y)[,nrow(Y):1], main = "Original Y", axes = FALSE, col = hcl.colors(12, "YlOrRd", rev = TRUE))
image(t(res$XB)[,nrow(res$XB):1], main = "Fitted XB", axes = FALSE, col = hcl.colors(12, "YlOrRd", rev = TRUE))
```

-----

## 2\. Handling Missing Values

One of the powerful features of `nmfkc` is its ability to handle missing values (`NA`). The algorithm automatically detects `NA`s and ignores them during optimization (conceptually assigning them a weight of 0).

### Creating Data with Missing Values

Let's introduce a missing value into our matrix `Y`.

```{r create-na-data}
Y_missing <- Y
Y_missing[1, 2] <- NA # Introduce a missing value
print(Y_missing)
```

### Running NMF with NAs

We simply pass the matrix with `NA`s to `nmfkc`. No extra arguments are needed.

```{r run-nmfkc-na}
# Run NMF on data with missing values
res_na <- nmfkc(Y_missing, rank = 2, seed = 123)
```

### Reconstruction / Imputation

The model estimates the latent structure ($X$ and $B$) using only the observed data. The fitted matrix ($XB$) effectively provides an **imputation** for the missing value.

```{r check-imputation}
# The fitted value for the missing entry [1, 2]
predicted_val <- res_na$XB[1, 2]
actual_val <- Y[1, 2]

cat(paste("Original Value: ", actual_val, "\n"))
cat(paste("Imputed Value:  ", round(predicted_val, 4), "\n"))
```

The model successfully reconstructed the missing value using the latent structure learned from the rest of the data.

