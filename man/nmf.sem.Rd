% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nmfkc.R
\name{nmf.sem}
\alias{nmf.sem}
\title{NMF-SEM Main Estimation Algorithm}
\usage{
nmf.sem(
  Y1,
  Y2,
  rank = NULL,
  X.init = NULL,
  X.L2.ortho = 100,
  C1.L1 = 1,
  C2.L1 = 0.1,
  epsilon = 1e-06,
  maxit = 20000,
  seed = 123,
  ...
)
}
\arguments{
\item{Y1}{A non-negative numeric matrix of endogenous variables with
\strong{rows = variables (P1), columns = samples (N)}.}

\item{Y2}{A non-negative numeric matrix of exogenous variables with
\strong{rows = variables (P2), columns = samples (N)}.
Must satisfy \code{ncol(Y1) == ncol(Y2)}.}

\item{rank}{Integer; number of latent factors \eqn{Q}. If \code{NULL},
\eqn{Q} is taken from a hidden argument in \code{...} or defaults to
\code{nrow(Y2)}.}

\item{X.init}{Optional non-negative initialization for the basis matrix
\code{X} (\eqn{P_1 \times Q}). If supplied, it is projected to be
non-negative and column-normalized.}

\item{X.L2.ortho}{L2 orthogonality penalty for \code{X}. This controls
the penalty term \eqn{\lambda_X \lVert X^\top X - \mathrm{diag}(X^\top X)
  \rVert_F^2}. Default: \code{100}.}

\item{C1.L1}{L1 sparsity penalty for \code{C1} (i.e., \eqn{\Theta_1}).
Default: \code{1.0}.}

\item{C2.L1}{L1 sparsity penalty for \code{C2} (i.e., \eqn{\Theta_2}).
Default: \code{0.1}.}

\item{epsilon}{Relative convergence threshold for the objective function.
Iterations stop when the relative change in reconstruction loss falls
below this value. Default: \code{1e-6}.}

\item{maxit}{Maximum number of iterations for the multiplicative updates.
Default: \code{20000}.}

\item{seed}{Random seed used to initialize \code{X}, \code{C1}, and \code{C2}.
Default: \code{123}.}

\item{...}{Additional arguments. Currently used to pass a hidden rank
\code{Q} (e.g., via \code{Q = 3}) if \code{rank} is \code{NULL}.}
}
\value{
A list with components:
\item{X}{Estimated basis matrix (\eqn{P_1 \times Q}).}
\item{C1}{Estimated latent feedback matrix (\eqn{\Theta_1}, \eqn{Q \times P_1}).}
\item{C2}{Estimated exogenous loading matrix (\eqn{\Theta_2}, \eqn{Q \times P_2}).}
\item{XC1}{Feedback matrix \eqn{X \Theta_1}.}
\item{XC2}{Direct-effect matrix \eqn{X \Theta_2}.}
\item{XC1.radius}{Spectral radius \eqn{\rho(X \Theta_1)}.}
\item{XC1.norm1}{Induced 1-norm \eqn{\lVert X \Theta_1 \rVert_{1,\mathrm{op}}}.}
\item{Leontief.inv}{Leontief-type inverse \eqn{(I - X \Theta_1)^{-1}.}}
\item{M.model}{Equilibrium mapping
\eqn{M_{\mathrm{model}} = (I - X \Theta_1)^{-1} X \Theta_2}.}
\item{amplification}{Latent amplification factor
\eqn{\lVert M_{\mathrm{model}} \rVert_{1,\mathrm{op}} /
         \bigl\lVert X \Theta_2 \bigr\rVert_{1,\mathrm{op}}}.}
\item{amplification.bound}{Geometric-series upper bound
\eqn{1 / (1 - \lVert X \Theta_1 \rVert_{1,\mathrm{op}})} if
\eqn{\lVert X \Theta_1 \rVert_{1,\mathrm{op}} < 1}, otherwise \code{Inf}.}
\item{Q}{Effective latent dimension used in the fit.}
\item{SC.cov}{Correlation between sample and model-implied covariance
(flattened) of \eqn{Y_1}.}
\item{MAE}{Mean absolute error between \eqn{Y_1} and its equilibrium
prediction \eqn{\hat Y_1 = M_{\mathrm{model}} Y_2}.}
\item{objfunc}{Vector of reconstruction losses per iteration.}
\item{objfunc.full}{Vector of penalized objective values per iteration.}
\item{iter}{Number of iterations actually performed.}
}
\description{
Fits the NMF-SEM model
\deqn{
  Y_1 \approx X \bigl( \Theta_1 Y_1 + \Theta_2 Y_2 \bigr)
}
under non-negativity constraints with orthogonality and sparsity regularization.
The function returns the estimated latent factors, structural coefficient matrices,
and the implied equilibrium (inputâ€“output) mapping.

At equilibrium, the model can be written as
\deqn{
  Y_1 \approx (I - X \Theta_1)^{-1} X \Theta_2 Y_2
  \equiv M_{\mathrm{model}} Y_2,
}
where \eqn{M_{\mathrm{model}} = (I - X \Theta_1)^{-1} X \Theta_2} is a
Leontief-type cumulative-effect operator in latent space.

Internally, the latent feedback and exogenous loading matrices are stored as
\code{C1} and \code{C2}, corresponding to \eqn{\Theta_1} and \eqn{\Theta_2},
respectively.
}
