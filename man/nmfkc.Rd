% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nmfkc.R
\name{nmfkc}
\alias{nmfkc}
\title{Optimize NMF with kernel covariates}
\source{
Satoh, K. (2024). Applying Non-negative Matrix Factorization with Covariates
to the Longitudinal Data as Growth Curve Model. arXiv:2403.05359.
\url{https://arxiv.org/abs/2403.05359}
}
\usage{
nmfkc(
  Y,
  A = NULL,
  Q = 2,
  gamma = 0,
  epsilon = 1e-04,
  maxit = 5000,
  method = "EU",
  X.restriction = "colSums",
  nstart = 1,
  seed = 123,
  prefix = "Basis",
  print.trace = FALSE,
  print.dims = TRUE,
  save.time = TRUE,
  save.memory = FALSE,
  fast.calc = FALSE
)
}
\arguments{
\item{Y}{Observation matrix.}

\item{A}{Covariate matrix. Default is \code{NULL} (no covariates).}

\item{Q}{Rank of the basis matrix \eqn{X}; must satisfy \eqn{Q \le \min(P,N)}.}

\item{gamma}{Nonnegative penalty parameter controlling
the ridge regularization term \eqn{\gamma\,\mathrm{tr}(C A A^\top C^\top)}.}

\item{epsilon}{Positive convergence tolerance.}

\item{maxit}{Maximum number of iterations.}

\item{method}{Objective function: Euclidean distance \code{"EU"} (default) or Kullback–Leibler divergence \code{"KL"}.}

\item{X.restriction}{Constraint for columns of \eqn{X}:
\code{"colSums"} (default; each column sums to 1),
\code{"colSqSums"} (each column has unit \eqn{\ell_2} norm), or
\code{"totalSum"} (entries sum to 1).}

\item{nstart}{Number of random starts for \code{\link[stats]{kmeans}} when initializing \eqn{X}.}

\item{seed}{Integer seed passed to \code{\link[base]{set.seed}}.}

\item{prefix}{Prefix for column names of \eqn{X} and row names of \eqn{B}.}

\item{print.trace}{Logical. If \code{TRUE}, prints progress every 10 iterations.}

\item{print.dims}{Logical. If \code{TRUE} (default), prints matrix dimensions and elapsed time.}

\item{save.time}{Logical. If \code{TRUE} (default), skips some post-computations (e.g., CPCC, silhouette) to save time.}

\item{save.memory}{Logical. If \code{TRUE}, performs only essential computations (implies \code{save.time = TRUE}) to reduce memory usage.}

\item{fast.calc}{Logical.
If \code{TRUE}, uses an optimized computation mode for large-scale matrices.
Specifically, for the Euclidean (\code{"EU"}) method with covariates \code{A},
precomputes \eqn{Y A^\top} and \eqn{A A^\top} to eliminate explicit dependence on the sample size \eqn{N}
during each iteration.
For the Kullback–Leibler (\code{"KL"}) method, automatically applies internal column blocking
to avoid constructing large \eqn{P \times N} ratio matrices.
If \code{FALSE} (default), runs the reference implementation with the original update rules.}
}
\value{
A list with components:
\item{call}{The matched call, as captured by \code{match.call()}.}
\item{dims}{A character string summarizing the matrix dimensions of the model.}
\item{X}{Basis matrix. Column normalization depends on \code{X.restriction}.}
\item{B}{Coefficient matrix \eqn{B = C A}.}
\item{XB}{Fitted values for \eqn{Y}.}
\item{C}{Parameter matrix.}
\item{B.prob}{Soft-clustering probabilities derived from columns of \eqn{B}.}
\item{B.cluster}{Hard-clustering labels (argmax over \eqn{B.prob} for each column).}
\item{X.prob}{Row-wise soft-clustering probabilities derived from \eqn{X}.}
\item{X.cluster}{Hard-clustering labels (argmax over \eqn{X.prob} for each row).}
\item{objfunc}{Final objective value.}
\item{objfunc.iter}{Objective values by iteration.}
\item{r.squared}{Coefficient of determination \eqn{R^2} between \eqn{Y} and \eqn{X B}.}
\item{sigma}{The residual standard error, representing the typical deviation of the observed values \eqn{Y} from the fitted values \eqn{X B}.}
\item{criterion}{A list of selection criteria, including \code{ICp}, \code{CPCC}, \code{silhouette}, \code{AIC}, and \code{BIC}.}
}
\description{
\code{nmfkc} fits a nonnegative matrix factorization with kernel covariates
under the tri-factorization model \eqn{Y \approx X C A = X B}, where
\eqn{Y(P,N)} is the observation matrix, \eqn{A(R,N)} is the covariate matrix,
\eqn{X(P,Q)} is the basis matrix (\eqn{Q \le \min(P,N)}), \eqn{C(Q,R)} is the
parameter matrix, and \eqn{B(Q,N)=C A} is the coefficient matrix.
Given \eqn{Y} and (optionally) \eqn{A}, the algorithm estimates \eqn{X} and \eqn{C}.

The estimation is based on minimizing a penalized objective function:
\deqn{
  J(X,C) =
  \begin{cases}
    \|Y - XCA\|_F^2 + \gamma\,\mathrm{tr}(C A A^\top C^\top), & \text{for method = "EU"},\\[6pt]
    \sum_{p,n}\!\bigl[-Y_{pn}\log(XCA)_{pn} + (XCA)_{pn}\bigr]
      + \gamma\,\mathrm{tr}(C A A^\top C^\top), & \text{for method = "KL"}.
  \end{cases}
}
When \code{A = NULL}, the penalty reduces to \eqn{\gamma\,\mathrm{tr}(C C^\top)}.
This ridge-type regularization on \eqn{C} (or \eqn{CA}) improves stability
and generalization by shrinking coefficient vectors toward zero.
}
\examples{
# install.packages("remotes")
# remotes::install_github("ksatohds/nmfkc")
# Example 1.
library(nmfkc)
X <- cbind(c(1,0,1),c(0,1,0))
B <- cbind(c(1,0),c(0,1),c(1,1))
Y <- X \%*\% B
rownames(Y) <- paste0("P",1:nrow(Y))
colnames(Y) <- paste0("N",1:ncol(Y))
print(X); print(B); print(Y)
library(nmfkc)
res <- nmfkc(Y,Q=2,epsilon=1e-6)
res$X
res$B

# Example 2.
Y <- matrix(cars$dist,nrow=1)
A <- rbind(1,cars$speed)
result <- nmfkc(Y,A,Q=1)
plot(cars$speed,as.vector(Y))
lines(cars$speed,as.vector(result$XB),col=2,lwd=2)
}
\references{
Ding, C., Li, T., Peng, W., & Park, H. (2006). Orthogonal Nonnegative Matrix Tri-Factorizations for Clustering.
In \emph{Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining} (pp. 126–135).
\doi{10.1145/1150402.1150420}
Potthoff, R. F., & Roy, S. N. (1964). A generalized multivariate analysis of variance model useful especially for growth curve problems.
\emph{Biometrika}, 51, 313–326. \doi{10.2307/2334137}
}
\seealso{
\code{\link{nmfkc.cv}}, \code{\link{nmfkc.rank}}, \code{\link{nmfkc.kernel}}, \code{\link{nmfkc.ar}}, \code{\link{predict.nmfkc}}
}
